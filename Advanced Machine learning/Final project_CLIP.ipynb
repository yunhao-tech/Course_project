{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLmhtCsD166J"
      },
      "source": [
        "#https://go.criteo.net/TP3ENSAE\n",
        "Solutions: https://colab.research.google.com/drive/1mFY63abCJazCsJwwPxoL_aqI0St0Tavw?usp=sharing\n",
        "\n",
        "#Goal\n",
        "- Understand Finetuning and try to match texts and images using CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3buHX9I16aW"
      },
      "source": [
        "# Finetuning and CLIP\n",
        "\n",
        "On many tasks, it is now common to initialize the network's weights using a similar architecture trained on some other data by modifying only the last layers. An example of how to do this is https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "The net used as a starting point is often designated as the \"backbone\", and the output of the backbone is named 'embedding'—the part doing the prediction the projection head.\n",
        " \n",
        "Last year a model named CLIP got a lot of attention. It is trained using very simple cosine matching, trying to reduce the distance between some embeddings of images and the embedding of their legend. A pre-trained version is available at https://github.com/openai/CLIP\n",
        "\n",
        "This sparkled a lot of derivative works, one of them beeing stable diffusion. \n",
        "\n",
        "\n",
        "Yet not necessary to answer the questions you may be interested in a [video explanation](https://www.youtube.com/watch?v=T9XSU0pKX2E) made by a researcher independent of the authors. \n",
        "\n",
        "\n",
        "\n",
        "**Q6.** CLIP claims to perform 0 shot classification by comparing the likelihood of several embedding of tuple (image, description). Adapt the Zero-shot example of the repository to perform 0-shot predictions on the Hymenoptera dataset. Do the same on Cifar-100.\n",
        "\n",
        "\n",
        "**Q7.** Adapt the finetuning code of Q5 such that the network model from CLIP can now be used as the backbone for a fine tuning on Cifar-100 and \n",
        "\n",
        "**Q8.** Perform a training trying to tune the learning rate with a SGD optimizer and freezing/unfreezing the CLIP network. Also try different learning rate for SGD on the frozen paramters and the added layers. Can you find some recommendations? \n",
        "\n",
        "**Q9.** On the Cifar-100 dataset create embedding of images and  corresponding texts using 0-shot prediction (which is precisely the setting given in the example). Plot a few of the images and their corresponding texts using 2d PCA made on their common embedding space with a color depending if the point is from an image or from a text. Do you have any comment? Are the text and image spaces organized similarly? \n",
        "\n",
        "**Q10.** Does the training of Q7 improves the situation detected at Q9?\n",
        "\n",
        "**Q11.** Try to improve the performances of Cifar-100 following recommendations of http://karpathy.github.io/2019/04/25/recipe/ and/or adding a multi-head attention layer instead of a single fully connected layer on the top of CLIP. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLfhVeX1t_7R",
        "outputId": "6e15b5f1-da68-4971-cafc-184f3252edfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  5 11:09:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NDlOpLACRtt"
      },
      "source": [
        "## Install CLIP and download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2de5Eng3h2Ie",
        "outputId": "07f3de6e-5356-445e-a1af-7095b33a0554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m748.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fb9upwp5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-fb9upwp5\n",
            "  Resolved https://github.com/openai/CLIP.git to commit 3702849800aa56e2223035bccd1c6ef91c704ca8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=26bbf0ceb047b21e14598636e19cc9ad78a4efa8fe39384d240cb960d7c2869f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8q79fqxt/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "# install CLIP\n",
        "!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.2\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POKgCuuFHB-R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100, ImageFolder\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "09f0c194acab4da6b704ff5ae18cddbf",
            "e1c7132874c743a787ad98117328f50d",
            "acd1dfdd5b324f80b2d85a92e5cef0b9",
            "59445b8d747d447babc703c71b0978b5",
            "ea6d1b26360c42d480699f040cb2e556",
            "1f7141ecdaee41deaa3b944abc7641f1",
            "10e3f1d86d334dcc9c38cff04a1efa33",
            "e9ea27d7cd3745f890aa5c1a28f12d86",
            "4f32aedcfdc3440c88c341ed70b3c848",
            "3dcde2a96f0f4cacbb49886ce23a8056",
            "f1a2fe238bbe4fa0a9d4cb6935c9e392"
          ]
        },
        "id": "dHxIpnPIuN2T",
        "outputId": "ace915db-1515-44b0-ae86-944f1e654182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 246MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09f0c194acab4da6b704ff5ae18cddbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "--2023-02-05 11:10:09--  https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.33.88.63, 13.33.88.36, 13.33.88.85, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.33.88.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47286322 (45M) [application/zip]\n",
            "Saving to: ‘./data/hymenoptera_data.zip’\n",
            "\n",
            "hymenoptera_data.zi 100%[===================>]  45.10M   107MB/s    in 0.4s    \n",
            "\n",
            "2023-02-05 11:10:10 (107 MB/s) - ‘./data/hymenoptera_data.zip’ saved [47286322/47286322]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download CIFAR dataset\n",
        "cifar100 = CIFAR100(root='./data', download=True, train=False, transform=preprocess)\n",
        "\n",
        "# download Hymenoptera dataset at https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "!wget -P './data' https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "!unzip -q './data/hymenoptera_data.zip' -d './data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOvd48NzCttg"
      },
      "source": [
        "## Q6: zero-shot predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QEHUwakJRK-"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWAIi9JJHWzD"
      },
      "source": [
        "In the first time, we perform zero-shot classification on a single sample, showing the top 5 predictions made by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bplclqVFu8m",
        "outputId": "e7652b97-0994-4101-c9c7-917518aa3d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The true label is motorcycle\n",
            "\n",
            "Top predictions:\n",
            "\n",
            "      motorcycle: 92.47%\n",
            "      lawn_mower: 4.04%\n",
            "         bicycle: 0.62%\n",
            "           plain: 0.50%\n",
            "           woman: 0.19%\n"
          ]
        }
      ],
      "source": [
        "input_index = 1096\n",
        "text_all_class = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_all_class)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  image, class_id = cifar100[input_index]\n",
        "  image_input = image.unsqueeze(0).to(device)\n",
        "  image_features = model.encode_image(image_input)\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "print(f\"The true label is {cifar100.classes[class_id]}\")\n",
        "\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM2bIgqTHwWM"
      },
      "source": [
        "We can notice that the legend prediction corresponds well to the input image (the correct answer gets more than 90% probability).\n",
        "\n",
        "Now, we compute the average prediction accuracy on the whole Cifar100 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyH70_aFDL3j"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, text_all_class):\n",
        "    n_correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_all_class)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        for images, labels in dataloader:\n",
        "          images = images.to(device)\n",
        "          image_features = model.encode_image(images)\n",
        "          image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "          similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "          _, indices = similarity.topk(1)\n",
        "          indices = indices.reshape(-1,)\n",
        "          n_correct += indices.eq(labels.to(device)).sum().item()\n",
        "          total += labels.size(0)\n",
        "    accuracy = n_correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_3KEPaZXDfa",
        "outputId": "69e48f89-efd5-4970-81b7-7ec477180153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For CIFAR100, The accuracy of zero-shot inference is 61.68%\n"
          ]
        }
      ],
      "source": [
        "dataloader = torch.utils.data.DataLoader(cifar100, batch_size=64,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "text_all_class = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "accuracy = evaluate(model, dataloader, text_all_class)\n",
        "print(f\"For CIFAR100, The accuracy of zero-shot inference is {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-NUqfURxLIl"
      },
      "source": [
        "Using ViT/B-32, we get an accuracy about **62%** by top1 similarity. If we refer to the [original paper of CLIP](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf), the accuracy of zero shot prediction on Cifar100 is **65.1%**.\n",
        "\n",
        "Here are some [discussions](https://github.com/openai/CLIP/issues/153). It seems that we have to ensemble the prompts to achieve such accuracy in paper, instead of using one single template of prompt.\n",
        "\n",
        "It proves that the quality of prompts could be a main factor of model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w7jvziyJgrq"
      },
      "source": [
        "### Hymenoptera dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOAkdwt3JOS1"
      },
      "outputs": [],
      "source": [
        "# refer to tutorial at https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "data_dir = './data/hymenoptera_data'\n",
        "image_datasets = ImageFolder(os.path.join(data_dir, 'train'),\n",
        "                                          preprocess)\n",
        "text_all_class_hyme = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in ['ant', 'bee'] ]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfXYip7TKZxJ",
        "outputId": "105fb16b-daa6-4841-f6e5-648aca5708be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The true label is ant\n",
            "\n",
            "Top predictions:\n",
            "\n",
            "             ant: 99.41%\n",
            "             bee: 0.57%\n"
          ]
        }
      ],
      "source": [
        "input_index = 24\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_all_class_hyme)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  image_input_, class_id = image_datasets[input_index]\n",
        "  image_input = (image_input_).unsqueeze(0).to(device)\n",
        "  image_features = model.encode_image(image_input)\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(2)\n",
        "\n",
        "print(f\"The true label is {['ant', 'bee'][class_id]}\")\n",
        "\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{['ant', 'bee'][index]:>16s}: {100 * value.item():.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTTq8mlQMGvf"
      },
      "source": [
        "On a randomly chosen sample, CLIP well predicts its class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMkFAc5HXkR7",
        "outputId": "d3cfcd56-8fc1-4d70-cb61-bd1dafa05b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Hymenoptera dataset, The accuracy of zero-shot inference is 95.49%\n"
          ]
        }
      ],
      "source": [
        "dataloader_hyme = torch.utils.data.DataLoader(image_datasets, batch_size=64,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "text_all_class_hyme = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in ['ant', 'bee'] ]).to(device)\n",
        "accuracy = evaluate(model, dataloader_hyme, text_all_class_hyme)\n",
        "print(f\"For Hymenoptera dataset, The accuracy of zero-shot inference is {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyarPyyADD7j"
      },
      "source": [
        "## Q7: Fine tuning on CIFAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpsID4KJts9-"
      },
      "source": [
        "Fine tuning on CLIP (based on Vision Transformer Base, with patch size 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYBCCsObSoCE",
        "outputId": "24974e66-a246-48d9-bc5b-d900ddd7fcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "model_finetune, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "\n",
        "# Load the dataset\n",
        "root = './data'\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkdoXiZCgoF_"
      },
      "source": [
        "Adapt the fine tuning code from the [discussion](https://github.com/openai/CLIP/issues/83). CLIP uses mix-precision training: float32 for optimization and float16 for forward/backward, as discussed [here](https://github.com/openai/CLIP/issues/57). \n",
        "\n",
        "Thus, we need to convert parameters and grads to float32 before applying optimizer, then convert it back. See it in the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNS3o5uI_LB0",
        "outputId": "60aae434-f333-4c11-8c1a-d4d4c933bf89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: training accuracy is 75.094% || test accuracy is 68.820%\n",
            "Epoch 2: training accuracy is 81.476% || test accuracy is 71.420%\n",
            "Epoch 3: training accuracy is 85.228% || test accuracy is 73.630%\n"
          ]
        }
      ],
      "source": [
        "text_all_class = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "loss_img = torch.nn.CrossEntropyLoss()\n",
        "loss_txt = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_finetune.parameters(), lr=5e-5)\n",
        "train_epoch = 3\n",
        "\n",
        "model_finetune.train()\n",
        "for epoch in range(train_epoch):  \n",
        "    for inputs, labels in trainloader:\n",
        "        texts = torch.cat([clip.tokenize(f\"a photo of a {cifar100.classes[i]}\") for i in np.array(labels)]).to(device)\n",
        "        images = inputs.to(device) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_per_image, logits_per_text  = model_finetune(images, texts)\n",
        "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "        total_loss.backward()\n",
        "\n",
        "        if device == \"cpu\":\n",
        "          optimizer.step()\n",
        "        else:\n",
        "          # convert the weights and to float32 \n",
        "          model_finetune.float()\n",
        "          optimizer.step()\n",
        "          # convert back to float16\n",
        "          clip.model.convert_weights(model_finetune)\n",
        "\n",
        "    train_acc = evaluate(model_finetune, trainloader, text_all_class)    \n",
        "    test_acc = evaluate(model_finetune, testloader, text_all_class)\n",
        "    print(f\"Epoch {epoch+1}: training accuracy is {train_acc*100:.3f}% || test accuracy is {test_acc*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"There are {n_params:,} parameters in CLIP ViT-B/32 model.\")\n",
        "print(f\"There are {len(train)} training samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtr14UvfoEXu",
        "outputId": "f8d22e30-db1e-4635-833b-66a4db822cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 151,277,313 parameters in CLIP ViT-B/32 model.\n",
            "There are 50000 training samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more than 150 millions trainable parameters. That's why the fine-tune is really slow (more than 10 minutes for 3 epochs of 50,000 traning samples). "
      ],
      "metadata": {
        "id": "FCOfNFd8oSxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWxs_13oU5Vl"
      },
      "outputs": [],
      "source": [
        "# save the model \n",
        "\n",
        "torch.save({\n",
        "        'epoch': train_epoch,\n",
        "        'model_state_dict': model_finetune.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, \"./CLIP_ckpt.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMvcee0_VQZK"
      },
      "outputs": [],
      "source": [
        "# reload the fine-tuned model\n",
        "\n",
        "# model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)\n",
        "# checkpoint = torch.load(\"./CLIP_ckpt.pt\")\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7QSj_HWI0GM"
      },
      "source": [
        "## Q8: Tune the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh_gBsiiI-py",
        "outputId": "111b2f18-75b4-4c27-f9a4-f736db7e174c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model_sgd, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "\n",
        "# Download CIFAR dataset\n",
        "cifar100 = CIFAR100(root='./data', download=True, train=False, transform=preprocess)\n",
        "\n",
        "# Load the dataset\n",
        "root = './data'\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPn3hYQkJPlC"
      },
      "outputs": [],
      "source": [
        "text_all_class = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "def tune_lr_sgd(trainloader, testloader, text_all_class, lr, train_epoch = 5):\n",
        "  model_sgd, _ = clip.load('ViT-B/32', device, jit=False)\n",
        "  loss_img = torch.nn.CrossEntropyLoss()\n",
        "  loss_txt = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = torch.optim.SGD(model_sgd.parameters(), lr)\n",
        "\n",
        "  model_sgd.train()\n",
        "  train_accs = np.zeros(train_epoch)\n",
        "  test_accs = np.zeros(train_epoch)\n",
        "  for epoch in range(train_epoch):  \n",
        "      for inputs, labels in trainloader:\n",
        "          texts = torch.cat([clip.tokenize(f\"a photo of a {cifar100.classes[i]}\") for i in np.array(labels)]).to(device)\n",
        "          images = inputs.to(device) \n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          logits_per_image, logits_per_text  = model_sgd(images, texts)\n",
        "          ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "          total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "          total_loss.backward()\n",
        "\n",
        "          if device == \"cpu\":\n",
        "            optimizer.step()\n",
        "          else:\n",
        "            # convert the weights and to float32 \n",
        "            model_sgd.float()\n",
        "            optimizer.step()\n",
        "            # convert back to float16\n",
        "            clip.model.convert_weights(model_sgd)\n",
        "\n",
        "      train_accs[epoch] = evaluate(model_sgd, trainloader, text_all_class)    \n",
        "      test_accs[epoch] = evaluate(model_sgd, testloader, text_all_class)\n",
        "      # print(f\"Epoch {epoch+1}: training accuracy is {train_acc*100:.3f}% || test accuracy is {test_acc*100:.3f}%\") \n",
        "  return train_accs, test_accs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lrs = [1e-2, 1e-3, 1e-5]\n",
        "accs = {}\n",
        "for i, lr in enumerate(lrs):\n",
        "  accs[i] = tune_lr_sgd(trainloader, testloader, text_all_class, lr, 3)"
      ],
      "metadata": {
        "id": "juLUxCeZhoqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(3)\n",
        "for i in range(3):\n",
        "  plt.plot(x, accs[i][0], label=f\"lr={lrs[i]}\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"#Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training accuracy for different SGD lr\", fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "for i in range(3):\n",
        "  plt.plot(x, accs[i][1], label=f\"lr={lrs[i]}\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"#Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Test accuracy for different SGD lr\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "7GKcNUmDEsKC",
        "outputId": "5d975206-4560-4c4f-b4c6-6e14b0c414d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: when the learning rate is too large, like in the case lr=0.01, the performance of the CLIP is very poor and does not improve through training epochs. What's more, the accuracy where lr=1e-3 is higher than the accuracy where lr=1e-5 after 3 training epochs, so the performance does not necessarily improve when we take smaller learning rates.\n",
        "\n",
        "**Recommendation**: When we fine-tune the big models (like CLIP), the learning rate should not be too big. That would lose the important information contained in the model and degrade the performance. Choose an appropriate learning rate is important. But it seems like a problem-specific task which has no universal principals."
      ],
      "metadata": {
        "id": "g7AE7GlzLM91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8: Freezing CLIP and add layers"
      ],
      "metadata": {
        "id": "Muy5eVN2lS74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'd like to add some fully connected layers on top of CLIP. \n",
        "\n",
        "One common way to do that is to remove the last layer in the first time (or just set them as `Identity()`); then add some fully connected layers as \"classifier head\". However, in CLIP, it is not so easy. \n",
        "\n",
        "In `forward` function of CLIP's source code, the blocks are not sequentially used: `self.visual` and `self.transformer` are called in parallel, respectively in `encode_image` and `encode_text`. Thus, adding sequentially the fully connected layers would not work here. \n",
        "\n",
        "Inspired by source code, we apply fully connected layers on images features and text features. Then we compute their similarities, just as in source code."
      ],
      "metadata": {
        "id": "rZ-FO_rREzqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCLIP(torch.nn.Module):\n",
        "    def __init__(self, model_CLIP):\n",
        "        super().__init__()\n",
        "        self.model = model_CLIP\n",
        "        self.fc1 = torch.nn.Linear(512, 512, dtype=torch.float16)\n",
        "        self.fc2 = torch.nn.Linear(512, 512, dtype=torch.float16)\n",
        "\n",
        "    def forward(self, imgs, text):\n",
        "        # x1, x2 = self.model(imgs, text)\n",
        "        # x1 = self.fc1(x1)\n",
        "        # x1 = self.fc2(x1)\n",
        "        # x2 = x1.t()\n",
        "        image_features = self.model.encode_image(imgs)\n",
        "        image_features = self.fc1(image_features)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        text_features = self.model.encode_text(text)\n",
        "        text_features = self.fc2(text_features)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.model.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "        return logits_per_image, logits_per_text\n",
        "    \n",
        "    def evaluate(self, dataloader, text_all_class):\n",
        "        n_correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            text_features = self.model.encode_text(text_all_class)\n",
        "            text_features = self.fc2(text_features)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            for images, labels in dataloader:\n",
        "              images = images.to(device)\n",
        "              image_features = self.model.encode_image(images)\n",
        "              image_features = self.fc1(image_features)\n",
        "              image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "              similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "              _, indices = similarity.topk(1)\n",
        "              indices = indices.reshape(-1,)\n",
        "              n_correct += indices.eq(labels.to(device)).sum().item()\n",
        "              total += labels.size(0)\n",
        "        accuracy = n_correct / total\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "T89asEglHGkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "myModel = MyCLIP(model)\n",
        "myModel = myModel.to(device)\n",
        "\n",
        "# Load the dataset\n",
        "batch_size = 64\n",
        "root = './data'\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVFdvmqYICcV",
        "outputId": "755ba82d-f326-4a24-fc10-7f4a6e453468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we freeze the CLIP network\n",
        "for para in myModel.model.parameters():\n",
        "    para.requires_grad = False"
      ],
      "metadata": {
        "id": "55T0t1MCK4jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch = 5\n",
        "optimizer = torch.optim.SGD(myModel.parameters(), lr=1e-3)\n",
        "loss_img = torch.nn.CrossEntropyLoss()\n",
        "loss_txt = torch.nn.CrossEntropyLoss()\n",
        "text_all_class = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "for epoch in range(train_epoch):  \n",
        "    for inputs, labels in trainloader:\n",
        "        texts = torch.cat([clip.tokenize(f\"a photo of a {cifar100.classes[i]}\") for i in np.array(labels)]).to(device)\n",
        "        images = inputs.to(device) \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_per_image, logits_per_text  = myModel(images, texts)\n",
        "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_acc = myModel.evaluate(trainloader, text_all_class)    \n",
        "    test_acc = myModel.evaluate(testloader, text_all_class)\n",
        "    print(f\"Epoch {epoch+1}: training accuracy is {train_acc*100:.3f}% || test accuracy is {test_acc*100:.3f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o1axuxQK9Nl",
        "outputId": "46329f6c-c3cc-461c-ac3f-16d954e8d65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: training accuracy is 59.216% || test accuracy is 58.650%\n",
            "Epoch 2: training accuracy is 67.576% || test accuracy is 66.660%\n",
            "Epoch 3: training accuracy is 71.200% || test accuracy is 69.700%\n",
            "Epoch 4: training accuracy is 72.948% || test accuracy is 71.180%\n",
            "Epoch 5: training accuracy is 74.254% || test accuracy is 72.440%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SGD optimizer with learning rate 1e-3 and by training only the last linear layer added on top of CLIP, we obtained a similar accuracy after 5 epochs compared with the results obtained by fine tuning the CLIP on CIFAR. \n",
        "\n",
        "We would recommend adding some layers as classifier head to the CLIP network, freezing the CLIP and training only the added layers instead of the big network CLIP, since it would be much more faster."
      ],
      "metadata": {
        "id": "ZSQLpDgCeOMC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYTKh8ZTGo7b"
      },
      "source": [
        "## Q9: 2d PCA on common embedding space for images and texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4HK97JJfG39"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LL12ZXba1TR"
      },
      "outputs": [],
      "source": [
        "def plot_2d_embeddings(model, dataset):\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "    ## Take a few samples (one batch)\n",
        "    one_batch = next(iter(dataloader))\n",
        "    imgs, labels = one_batch\n",
        "    texts = torch.cat([clip.tokenize(f\"a photo of a {dataset.classes[i]}\") for i in np.array(labels)]).to(device)\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    image_features = model.encode_image(imgs)\n",
        "    text_features = model.encode_text(texts)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # fit the PCA model\n",
        "    pca = PCA(n_components=2)\n",
        "    pca.fit(torch.cat([image_features, text_features]).cpu().detach().numpy())\n",
        "    # Transform \n",
        "    image_features_2d = pca.transform(image_features.cpu().detach().numpy())\n",
        "    text_features_2d = pca.transform(text_features.cpu().detach().numpy())\n",
        "\n",
        "    plt.scatter(image_features_2d[:,0], image_features_2d[:,1], color='blue')\n",
        "    plt.scatter(text_features_2d[:,0], text_features_2d[:,1], color='red')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "1KfWW_UBfnzJ",
        "outputId": "84f99d40-b4a5-4719-9f53-16d530f50d8b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYCklEQVR4nO3df4wcZ33H8c/HdxwlDQHsuCFNYl9onUpuqUJ7TVshfrQYNbhVglQKiY70kFCt5BoJCSHVkiv+SGUJiEpbiThgQVWTHAoQFbCKKZBARFsRGqeEIIOCTYodpwFsB2ijUBw73/4xu731ZmZ/3MzOzM68X1J0O7NzO89jx9959vnxfRwRAgC0w7qqCwAAKA9BHwBahKAPAC1C0AeAFiHoA0CLzFZdgCwXXnhhzM/PV10MAJgqDz744MmI2Jj1fm2D/vz8vA4ePFh1MQBgqtg+Ouh9uncAoEUI+gDQIgR9AGiRQoK+7attP2L7iO2dKe/faPubth+y/a+2txZxXwDAeHIHfdszkm6T9AZJWyVdnxLUPxYRL4+IKyW9T9L7894XADC+Ilr6V0k6EhGPRsRpSXdJurb3goj4757Dn5dEljcA7bC8LM3OSnbyc3m50uIUMWXzEkmP9Rwfl/Tb/RfZ/nNJ75Q0J+n30z7I9g5JOyRp06ZNBRQNACq0vCzdfvvq8dmzq8d79lRSpNIGciPitoj4JUl/IekvM67ZGxELEbGwcWPm2gIAmA579453vgRFBP3HJV3Wc3xp51yWuyS9sYD7AkC9nT073vkSFBH0H5C0xfbltuckXSdpf+8Ftrf0HP6hpMMF3BcA6m1mZrzzJcgd9CPijKSbJX1e0rclfSIiDtm+xfY1nctutn3I9kNK+vWX8t4XAGpvx47xzpegkNw7EXFA0oG+c+/uef2OIu4DAFOlO1i7d2/SpTMzkwT8igZxpRonXAOARtizp9Ig3480DADQa2VFmp+X1q1Lfq6sVF2iQhH0AaBrZSXpfjl6VIpIfu7YMX7gr/GDg6APAF27dklPP33uuaefTs6PqqgHx4QQ9AGg69ix8c6nKeLBMUEEfQDoykr/Mk5amCIeHBNE0AeArt27pfPOO/fceecl50dVxINjggj6ANC1uJjMqd+8OcmKuXlzcry4OPpnFPHgmCDm6QNAr8XF8YJ82u9LSR/+sWNJC3/37nyfWSCCPgAULe+DY4Lo3gGAFiHoA0CLEPQBoEUI+gDQIgR9AGgRgj4AtAhBHwDWosaZNAdhnj4AjKubSbObWK2bSVOq7fz8Llr6ADCummfSHISgDwDjqnkmzUEI+gAwrppn0hyEoA8A46p5Js1BCPoAMK4iUjBXhNk7ALAWNc6kOQgtfQBoEYI+ALQIQR8AWoSgDwAtQtAHgElZXpZmZ5MZPrOzyXHFmL0DAJOwvCzdfvvq8dmzq8d79lRTJtHSB4DJ2Lt3vPMlKSTo277a9iO2j9jemfL+O21/y/bDtu+1vbmI+wJAbZ09O975kuQO+rZnJN0m6Q2Stkq63vbWvsu+LmkhIn5d0t2S3pf3vgBQazMz450vSREt/askHYmIRyPitKS7JF3be0FEfDkiunlI75d0aQH3BYD66ubXH/V8SYoI+pdIeqzn+HjnXJa3S/pc2hu2d9g+aPvgiRMnCigaAFRkzx7ppptWW/YzM8lxhYO4UskDubbfKmlB0q1p70fE3ohYiIiFjRs3llk0ACjenj3SmTNSRPKz4oAvFTNl83FJl/UcX9o5dw7b2yTtkvSaiPhZAfcFAIypiJb+A5K22L7c9pyk6yTt773A9iskfUjSNRHxwwLuCQBYg9xBPyLOSLpZ0uclfVvSJyLikO1bbF/TuexWSedL+qTth2zvz/i4WpnSze4BIFMhK3Ij4oCkA33n3t3zelsR9ynTFG92DwCZWr0id1BLfoo3uweATK3NvTOsJT/Fm90DQKbWtvSHteSneLN7AMjU2qA/rCU/xZvdA0Cm1gb9YS35Kd7sHgAytTbop7XkbWn79tXjxUXpe9+Tnn02+UnABzDtWhv0FxelpaUk0HdFSPv2MR8fQHO1NuhL0oEDSaDvxbRMAE3W6qDPtEwAbdPqoM+0TABt0+igPyx3DtMyAbRNY4N+d8Xt0aNJv313xW1v4GdaJoC2cfSPZNbEwsJCHDx4cM2/Pz+fBPp+mzcn0y8BoIlsPxgRC1nvN7alzyAtADxXY4M+g7QA8FyNDfqjDtKyUQqANmls0B9lkHaUwV4AaJLGDuRmWVlJVtweO5a07s+efe41DPYCmFbDBnJbtYlK/8YpaQFfYrAXQHM1tnsnTdrGKWkY7AXQVK0K+qO04FmRC6DJWhX0s1rwMzOsyAXQDq0K+lnTOPftY6MUAO3QqqBPrh0AbdeqoC+xBSKACtRoFWirpmwCQOn654p3V4FKlbQ6W9fSB4BSpc0Vr3BfVoI+AExS1lzxo0cr6eZpfNCvUVcagDYatNqzgmRfjQ76JFQDULm0ueJdFXTzFBL0bV9t+xHbR2zvTHn/1bb/w/YZ228q4p6jqFlXGoA26s4Vz1Jysq/cQd/2jKTbJL1B0lZJ19ve2nfZMUlvk/SxvPcbB7tnAaiFxcVkYVCakpN9FdHSv0rSkYh4NCJOS7pL0rW9F0TE9yLiYUnPFnC/kRWxexZjAgAKMerOThNWRNC/RNJjPcfHO+cql/fPmDEBAIWpSUqAWg3k2t5h+6DtgydOnMj9eXn/jBkTAFCoGqQEKCLoPy7psp7jSzvnxhYReyNiISIWNm7cWEDR8v0ZMyYAYGIq6jsuIug/IGmL7cttz0m6TtL+Aj53zYr6syxiTAAAnqPCvuPcQT8izki6WdLnJX1b0ici4pDtW2xfI0m2f8v2cUl/IulDtg/lvW+WIv8sazLuAqBpKuw7btzG6PPzSaDvt9bNzns3Ut+0KQn4ZOYEkMu6dUmrtJ+d9EXn0LqN0Yvuh19cJMgDKNimTemt0xL6jms1e6cI9MMDqL3t25NWfa+S+o4bF/TphwdQaysryR6tvd07trS0VEq3QuOCfk3WPwBAurRB3AjpwIFSbt+4gVwAqLUJDuImHzN4ILdxLX0AqLWKBx4J+gBQpooHHgn6AFCmigceWxH0SY8MoFYqTLzWuMVZ/bppGbqD5d20DBIzegC0T+Nb+uOmuOBbAYAma3xLf5y0DHwrANB0jW/pjzM7ik1TADRd44P+OLOj2DQFQKkq6E9ufNAfZ3YUydoAlKaijVQaH/SlwbOjeh+0Tz0lPe955/4uydoATERF/cmtCPpZ+h+0p04l3wY2bCBZG4AJq6g/ufGzdwZJe9CePi2df7508mQ1ZQLQEhVtpNLqln7WA/XoUeboA5iwinLwtDroD3qglrg5PYA2qigHT6vz6fcvxkqz1g3VAaAK5NMfoPdBm4U5+gCapNVBX1qdzpkV+JmjD6BJWh/0u9hQHUAbEPQ72FAdQBu0ep5+v8VFgjyAZqOlDwAtQtAHgDooKeMm3TsAULUSd3CipQ8AVSsx4yZBHwCqVmLGzUKCvu2rbT9i+4jtnSnvP9/2xzvvf832fBH3HRWbnQOotRJ3cMod9G3PSLpN0hskbZV0ve2tfZe9XdKPIuKXJf2NpPfmve+oKtqcBgBGV+Lq0CJa+ldJOhIRj0bEaUl3Sbq275prJe3rvL5b0utsu4B7D8Vm5wBqr8TVoUUE/UskPdZzfLxzLvWaiDgj6SeSNvR/kO0dtg/aPnjixIkCisZm5wBqYlg/86B9XQtUq4HciNgbEQsRsbBx48ZCPpPNzgFUrkb9zEUE/cclXdZzfGnnXOo1tmclvUjSqQLuPRSJ1ABUrkb9zEUE/QckbbF9ue05SddJ2t93zX5JS53Xb5L0pZjg7i2936J27ZKWlkikBqBCNepnzr0iNyLO2L5Z0uclzUj6+4g4ZPsWSQcjYr+kj0i6w/YRSU8qeTBMRNrCto98RHrhCyd1RwAYoqJN0NM0brvE+fn0P9te551Hax9AidL2Zp2bS1qjTz6ZBP/duwsJSq3bLnGUb0tM2QRQqv4pmRs2JAO6p06VPrDbuKA/6rel7sOB1boAchk1iPROyTz/fOmZZ859v6TWaOOCftpsnTTr19dqFhWAabTWIFLhwG7jgn7/t6h1A2pYo1lUAKbRWoNIhQuIGhf0pXO/RWWNUz/5ZK1mUQGYRmsNIhUuIGpk0O816IHKal0Auaw1iJSYa6df44P+oAcqq3UB5JIniJSUa6df44P+oAdqhQ9bAE0whUGkcYuzAKDNWrc4CwAqV+MFQLlz7wAAeqQlANuxI3ldg24fWvoAkGXUFnvvdUtLtV4AREsfANKM2mLvv+7s2fTPq8kCIFr6AJBm1NW2adelqckCIII+AKQZdbXtKC34Gi0AIugDQJpRV9tmXTczU8u5+wR9AEgz6mrbrOv27St9te0oCPoAkGbU1bZTtiq39StyV1aScZhjxwrdsQwAKjFsRW6rp2zWfA0FABSu1d07bKICoG0aF/S7C+NsaXY2+Zm1kI5NVAC0TaOCfu92ldLqwrhut83y8rkrqtevT/+cmqyhANA2JSRqa1Sf/qCFcU8/LX3wg6vbJx49Ks3NSc973rmb0tdoDQWANilpkLFRLf1h3TL9E5VOn5YuuGBqZloBaLKSBhkb1dLftGm1a2dUTz4pnTw5mfIAwMhKGmRsVEs/bWHcMPTfA6iFtW6yPqZGBf3ehXFSkvpCSrpusmzfnn6+xhvfAGiiPJusj6FRQV9a3WA+QjpzJnkADFp0fODAc88tL0s33JB0FUWsjqcQ+AFMTEnpHBqfhmHdusFB305yInWtrCQBP+13Nm9OHigAUFet3xh9WHdY//u7dmU/JFi0BWBiSupTzhX0ba+3/UXbhzs/X5Jx3T/b/rHtf8pzv7UYNLib1l02KLAz6AtgInpXlk64TzlvS3+npHsjYoukezvHaW6VdEPOe63J4mKyT3F3ULcrq7ssK7DbLNoCkFNWa77ERGB5g/61kvZ1Xu+T9Ma0iyLiXkn/k/Nea7Kykuxl0LtX8dyc9NRTSd99/7eotG8GtnTjjSzaApDDoNZ8iYnA8gb9iyLiic7r70u6KM+H2d5h+6DtgydOnMhZtETaA/T0aenUqfRvUWkD6HfcIe3ZU0hxALTVoNZ8SXP0pRFm79i+R9JLU97aJWlfRLy459ofRURWv/5rJb0rIv5olIKVNXuni5k5ACYqKxjZScuyN++OlHQ5rGHKZu5NVCJi24AP/4HtiyPiCdsXS/rhWKUrwaipGZiZA2CisoLRpk2rgb2Ebfzydu/sl7TUeb0k6TM5P69wu3cPXpHbxcwcABM1bMVtd2XphDdTzxv03yPp9bYPS9rWOZbtBdsf7l5k+18kfVLS62wft/0HOe87ssXF4d07pFMGMHE12UC98StypWSGTlYXz+bNbIYOoDlavyJXyv5WdeedE/0WBQC104qgX5NvVQBQuUZtojLI4iJBHgBa0dJPQ758AG3UmpZ+r5L2HwaA2mllS7/E3EYAMNzysjQ7mww6zs4mxxPSypZ+ibmNAGCw5WXp9ttXj8+eXT2eQNKvVrX0u/34WUsTWJULoHR79453PqfWtPT7+/H7sSoXQCV6876Pcj6n1rT00/rxu5i3D6Ay/Ts8DTufU2uCflZ/vc2qXAAV6k4dHPV8Tq0J+kXtUcD8fgCF2rNHuumm1Zb9zExyPKGdm1oT9IdlNR1FiXsXA2iy/tbjK18pnTmTBJYzZya6VV9rgn4R+XeY3w8gt4pbj61IrVyUQbudPfts+eUBMIWycr0XtGcrqZULVOLexQCaKmtzj5JWhxL0x1DEuACAFltZyd6/taTWI0F/DOTlB5DLrl3ZKQG2by+lCPTpA0BZsgYGJfr0AaBxBnXh0KcPAA2zezd9+gDQGouL0o03PjfwlzgjhKAPAGXas0e6447KZoQQ9AGgbIuLyaDts8+em/GxhORercmnDwC1tbIiveMd0qlTq+cmtHk3LX0AmJRRWu7dXDy9Ab9rAsm9aOkDwCT0b9eX1XIftMOTVPhUTlr6ADAJo6blHRbUC57KSdAHgEnICub95wcF9QlM5SToA8AkjJqWNy2ToyRt2DCRqZy5gr7t9ba/aPtw5+dLUq650vZXbR+y/bDtt+S5JwBMhWFpebuDvDfcIL3gBUmQ787bv/NO6eTJiczdz9vS3ynp3ojYIuneznG/pyX9aUT8qqSrJf2t7RfnvC8A1NugtLz9u2edOiX99KfJoq3eefsTkCvLpu1HJL02Ip6wfbGk+yLiV4b8zjckvSkiDg+6jiybABplZSUZxD12LJnCefbsc68pINPmsCybeadsXhQRT3Ref1/SRUMKc5WkOUnfzXh/h6QdkrSJ7agANEX/9M20gC+VkmlzaNC3fY+kl6a8dc68o4gI25lfGzrfBO6QtBQRqTvKRsReSXulpKU/rGwAMBWGzcXvKqGxOzToR8S2rPds/8D2xT3dOz/MuO4CSZ+VtCsi7l9zaQFgGo3Sgi8p02begdz9kpY6r5ckfab/Attzkj4l6aMRcXfO+wHA9Mlqwc/MlJ5pM2/Qf4+k19s+LGlb51i2F2x/uHPNmyW9WtLbbD/U+e/KnPcFgOmRNX1z377nZtqcMPbIBYAy9M7e2bQpeRBMINBPevYOAGAUi4ulteYHIQ0DALQIQR8AWoSgDwAtQtAHgBYh6ANAi9R2yqbtE5KOZrx9oaSTJRanLE2sVxPrJDWzXk2sk9TMeg2q0+aI2Jj1i7UN+oPYPjhoHuq0amK9mlgnqZn1amKdpGbWK0+d6N4BgBYh6ANAi0xr0N9bdQEmpIn1amKdpGbWq4l1kppZrzXXaSr79AEAazOtLX0AwBoQ9AGgRaYi6Nteb/uLtg93fr4k47pNtr9g+9u2v2V7vtySjmfUenWuvcD2cdsfKLOM4xqlTravtP1V24dsP2z7LVWUdRS2r7b9iO0jtnemvP982x/vvP+1uv8/J41Up3d2/v08bPte25urKOe4htWr57o/th22az+Nc5Q62X5z5+/rkO2PDf3QiKj9f5LeJ2ln5/VOSe/NuO4+Sa/vvD5f0nlVl72IenXe/ztJH5P0garLnbdOkq6QtKXz+hclPSHpxVWXPaWcM5K+K+llkuYkfUPS1r5rliV9sPP6Okkfr7rcBdTp97r/diTdVPc6jVqvznUvlPQVSfdLWqi63AX8XW2R9HVJL+kc/8Kwz52Klr6kayXt67zeJ+mN/RfY3ippNiK+KEkR8VREjLATcaWG1kuSbP+mpIskfaGkcuUxtE4R8Z2IONx5/V9K9lbOXEFYoaskHYmIRyPitKS7lNSvV29975b0OtsusYzjGlqniPhyz7+d+yVdWnIZ12KUvytJ+itJ75X0v2UWbo1GqdOfSbotIn4kSRGRuk95r2kJ+hdFxBOd199XEgD7XSHpx7b/0fbXbd9qe6a8Iq7J0HrZXifpryW9q8yC5TDK39X/s32VklbMdyddsDW4RNJjPcfHO+dSr4mIM5J+ImlDKaVbm1Hq1Ovtkj430RIVY2i9bP+GpMsi4rNlFiyHUf6urpB0he1/s32/7auHfWhtds6yfY+kl6a8tav3ICLCdto801lJr5L0CknHJH1c0tskfaTYko6ngHotSzoQEcfr0oAsoE7dz7lY0h2SliLi2WJLibxsv1XSgqTXVF2WvDqNp/criQlNMquki+e1Sr6RfcX2yyPix4N+oRYiYlvWe7Z/YPviiHiiEyjSvsIcl/RQRDza+Z1PS/odVRz0C6jX70p6le1lJeMUc7afiojMgapJK6BOsn2BpM9K2hUR90+oqHk9LumynuNLO+fSrjlue1bSiySdKqd4azJKnWR7m5KH+Gsi4mcllS2PYfV6oaRfk3Rfp/H0Ukn7bV8TEXXdjHuUv6vjkr4WEc9I+k/b31HyEHgg60OnpXtnv6SlzuslSZ9JueYBSS+23e0b/n1J3yqhbHkMrVdELEbEpoiYV9LF89EqA/4IhtbJ9pykTympy90llm1cD0jaYvvyTpmvU1K/Xr31fZOkL0VnRK2mhtbJ9iskfUjSNaP0EdfEwHpFxE8i4sKImO/8W7pfSf3qGvCl0f7/+7SSVr5sX6iku+fRgZ9a9Qj1iKPYGyTdK+mwpHskre+cX5D04Z7rXi/pYUnflPQPkuaqLnsR9eq5/m2q/+ydoXWS9FZJz0h6qOe/K6sue0Z9tkv6jpIxh12dc7coCRiS9HOSPinpiKR/l/SyqstcQJ3ukfSDnr+b/VWXuYh69V17n2o+e2fEvysr6bb6VifuXTfsM0nDAAAtMi3dOwCAAhD0AaBFCPoA0CIEfQBoEYI+ALQIQR8AWoSgDwAt8n8XKFPxmi2q3gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_2d_embeddings(model, cifar100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h2REdqPdudH"
      },
      "source": [
        "The red points are text embeddings, the blue ones are image embeddings. \n",
        "\n",
        "In their common space, the text and imgages embeddings are \"symmetrically\" organised. Moreover, the text and image spaces are organized similarly: they have respectively the similar coordinate in x-axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnS-e-RvHzZB"
      },
      "source": [
        "## Q10: Explore the improvements brought by fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klBcYR_kfubb"
      },
      "source": [
        "To check the possible improvements brought by fine tuning, it is sufficient to replace the original model by the fine-tuned one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmHmM2NHn9oH",
        "outputId": "2cd6e8a9-594a-49d3-bb92-0402662a2741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "##### If there is fine-tuned model, no need to execute it #####\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B0pHy7aiACu",
        "outputId": "36bafdae-262f-4d9b-f42b-ab3538d426a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##### If there is fine-tuned model, no need to execute it #####\n",
        "\n",
        "# Get the checkpoint mounted at Google Drive\n",
        "checkpoint = torch.load(\"CLIP_ckpt.pt\")\n",
        "\n",
        "# Reload the model\n",
        "model_finetune, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "model_finetune.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6t5WST0iiGTf",
        "outputId": "945c6a22-520a-4d7a-a5bf-2335e760f62d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_2d_embeddings(model_finetune, cifar100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_aQVLgBmJ09"
      },
      "source": [
        "For text and images embeddings respectively, the restrictions on first coordinate (on x-axis) are more strict. That is to say, their respective x-axis are more similar. \n",
        "\n",
        "Other observation is that the text and image embeddings (the red points and blue ones) \"exchange\" their position after fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFx-2G3U61rq"
      },
      "source": [
        "#Additional  considerations and other great tools\n",
        "\n",
        "##Fast KNN\n",
        "It is common to build costly embeddings selecting one layer close to the end of a deep net as a representation. Then looking for similarities becomes a k nearest neighbors search. As the dimension is almost always >100, no exact implementation runs in sub-linear time.   Thus,  we often conduct an approximate one that uses hyperplanes to build hashes to scale. The library [FAISS](https://github.com/facebookresearch/faiss) proposes an efficient implementation of this strategy. \n",
        "\n",
        "##Features stores\n",
        "Often a fast but inconvenient raw data source is used to store the data stream. Then some transformations are applied regularly to build the features used by the predictive algorithm. This comes with a problem: each new project needs new features making the transformation step more and more costly and possibly inconsistent over time. Feature store is a concept introduced in 2020 to answer this problem by versioning the feature extraction. Read more at https://eugeneyan.com/writing/feature-stores/\n",
        "\n",
        "##Discovering some of the deep learning concepts\n",
        "- Each article is a gem: https://distill.pub/\n",
        "\n",
        "- The [course of François Fleuret](https://fleuret.org/dlc/) is free and offers an excellent tour. \n",
        "\n",
        "\n",
        "##Empirical lessons from training deep nets\n",
        "In my experience, these are good practices when training deep nets: http://karpathy.github.io/2019/04/25/recipe/\n",
        "\n",
        "Nevertheless, since early 2021, the remark on self-supervised should be mitigated with respect to the successes of CLIP/ALIGN/BYOL. \n",
        "\n",
        "##Large dataset\n",
        "There exist specialized dataloaders to handle large data: \n",
        "- [Webdataset](https://github.com/webdataset/webdataset)\n",
        "- [Amazon S3 format](https://aws.amazon.com/fr/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/)\n",
        "\n",
        "Also, Tensorflow is more mature for serving while being more painful to use. Care about automatic translation from PyTorch using ONNX, which can strongly degrade the metrics of the network. Google researchers are currently often using [Jax](https://github.com/google/jax).\n",
        "\n",
        "##Demo time\n",
        "[Streamlit](https://streamlit.io/) and [Gradio](https://gradio.app/) are a super simple way to build an HTML interface to a python ML model. As a bonus, Gradio event uses the inputs to build automatically an API following the OpenAPI standards.  \n",
        "\n",
        "## Some food to think\n",
        "- As an average deep learner: https://marksaroufim.substack.com/p/working-class-deep-learner\n",
        "- As a Ph.D. student: https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8LO66FWye2"
      },
      "source": [
        "##More code examples\n",
        "\n",
        "Many examples are available on the net, here it is one code generator for basic classification with some intrumentation and parallel training.\n",
        "https://share.streamlit.io/pytorch-ignite/code-generator"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09f0c194acab4da6b704ff5ae18cddbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1c7132874c743a787ad98117328f50d",
              "IPY_MODEL_acd1dfdd5b324f80b2d85a92e5cef0b9",
              "IPY_MODEL_59445b8d747d447babc703c71b0978b5"
            ],
            "layout": "IPY_MODEL_ea6d1b26360c42d480699f040cb2e556"
          }
        },
        "e1c7132874c743a787ad98117328f50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f7141ecdaee41deaa3b944abc7641f1",
            "placeholder": "​",
            "style": "IPY_MODEL_10e3f1d86d334dcc9c38cff04a1efa33",
            "value": "100%"
          }
        },
        "acd1dfdd5b324f80b2d85a92e5cef0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ea27d7cd3745f890aa5c1a28f12d86",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f32aedcfdc3440c88c341ed70b3c848",
            "value": 169001437
          }
        },
        "59445b8d747d447babc703c71b0978b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dcde2a96f0f4cacbb49886ce23a8056",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a2fe238bbe4fa0a9d4cb6935c9e392",
            "value": " 169001437/169001437 [00:13&lt;00:00, 14929180.37it/s]"
          }
        },
        "ea6d1b26360c42d480699f040cb2e556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7141ecdaee41deaa3b944abc7641f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e3f1d86d334dcc9c38cff04a1efa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9ea27d7cd3745f890aa5c1a28f12d86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f32aedcfdc3440c88c341ed70b3c848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dcde2a96f0f4cacbb49886ce23a8056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a2fe238bbe4fa0a9d4cb6935c9e392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}