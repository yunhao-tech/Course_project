{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "<center><h2>ALTeGraD 2022<br>Lab Session 2: Transfer learning for NLP</h2> 27 / 10 / 2022<br> M. Kamal Eddine, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> [Yunhao CHEN]\n",
        "\n",
        "</center>\n",
        "\n",
        "<br><br>\n",
        "In this lab we will:\n",
        "* Implement and pretrain a language model with transformer architecture.\n",
        "* Use the pretrained model (transfer learning) to perform a sentiment analysis task which consists of classifying some books reviews into positive and negative ones.\n",
        "* Compare the performance of the pretrained model to a model trained from scratch.\n",
        " <br>\n",
        " \n",
        "<b>The deadline for this lab is November 14, 2022 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        '''\n",
        "        ntokens: the size of vocabulary\n",
        "        nhid: the hidden dimension of the model.\n",
        "        We assume that embedding_dim = nhid\n",
        "        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "        nhead: the number of heads in the multiheadattention models\n",
        "        dropout: the dropout value\n",
        "         '''\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.encoder = nn.Embedding(num_embeddings=ntoken, embedding_dim=nhid) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=nhid, nhead=nhead, dim_feedforward=nhid, dropout=dropout) #fill me we assume nhid = d_model = dim_feedforward\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers) #fill me\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) \n",
        "        src = self.pos_encoder(src) #fill me\n",
        "        output = self.transformer_encoder(src, src_mask) #fill me\n",
        "        return output\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid, nclasses):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid, nclasses) #fill me)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "    \n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken, nhead, nhid, nlayers, nclasses, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.base = TransformerModel(ntoken, nhead, nhid, nlayers) #fill me\n",
        "        self.classifier = ClassificationHead(nhid, nclasses) #fill me \n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # base model\n",
        "        x = self.base(src, src_mask) # fill me\n",
        "        # classifier model\n",
        "        output = self.classifier(x) #fill me\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, nhid, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, nhid)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEYHJx2JW6l"
      },
      "source": [
        "Let's verify if our model works, by applying one inference step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rhb2gkUhJMR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267c0fe7-8546-41bf-9eac-c882dbab122c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 100])\n"
          ]
        }
      ],
      "source": [
        "ntokens = 100 # the size of vocabulary\n",
        "nhid = 200  # hidden dimension\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\n",
        "dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\n",
        "src_mask = model.base.generate_square_subsequent_mask(1).to(device)\n",
        "out = model.forward(dummy_input, src_mask)\n",
        "\n",
        "print(out.shape) # is it the right shape?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5qjd26ghWuff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566eea0d-91b0-4bc1-828b-b375d48b8dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-07 18:42:08--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577587 (564K) [text/plain]\n",
            "Saving to: ‘dict.txt’\n",
            "\n",
            "dict.txt            100%[===================>] 564.05K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-11-07 18:42:10 (47.0 MB/s) - ‘dict.txt’ saved [577587/577587]\n",
            "\n",
            "▁d 1\n",
            "es 1\n",
            "▁l 1\n",
            "en 1\n",
            "on 1\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!head -5 dict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vFdH_-JeFbGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399f7e39-2c55-4b86-e16a-dd61bd654570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁ré\n"
          ]
        }
      ],
      "source": [
        "path_vocab = \"dict.txt\"\n",
        "token2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        token2ind[word] = idx + 4  #fill me\n",
        "\n",
        "ind2token = {value:key for key, value in token2ind.items()} #fill me\n",
        "\n",
        "print(ind2token[111])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents,\n",
        "        path_labels=None,\n",
        "        token2ind={},\n",
        "        max_len=512,\n",
        "        task=\"language_modeling\",\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1]\n",
        "        source_sequence = [self.token2ind[word] if word in self.token2ind.keys() else self.token2ind[\"<oov>\"] for word in sequence] #fill me (constract the input sequence using token2ind, sequence and special tokens)\n",
        "        if self.task == \"language_modeling\":\n",
        "            target = source_sequence[1:]\n",
        "            target.append(self.token2ind[\"<eos>\"])\n",
        "        elif self.task == \"classification\":\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            \"source_sequence\": torch.tensor(source_sequence),\n",
        "            \"target\": torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def MyCollator(batch):\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    path_documents,\n",
        "    path_labels=None,\n",
        "    token2ind={},\n",
        "    max_len=512,\n",
        "    batch_size=32,\n",
        "    task=\"language_modeling\",\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=512,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=MyCollator,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    path_data_train,\n",
        "    path_labels_train=None,\n",
        "    path_data_valid=None,\n",
        "    save_interval=-1,\n",
        "    log_interval=5,\n",
        "    task=\"language_modeling\",\n",
        "    batch_size=32,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    \n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): #step 1\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "        input = data[0].to(device)\n",
        "        output = model(input, src_mask) #step 2\n",
        "        if task == 'classification':\n",
        "            #last vector only\n",
        "            output = output[-1,:,:] #fill me \n",
        "          \n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = data[1] #fill me\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)  #fill me, Cross entropy check next cells\n",
        "        loss.backward()\n",
        "        #fill me step 3\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "        optimizer.step()\n",
        "        #fill me step 4\n",
        "\n",
        "        total_loss += loss.item() \n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
        "                \"loss {:5.5f} | ppl {:8.3f}\".format(\n",
        "                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n",
        "                )\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pgf6BDB9jUr6"
      },
      "outputs": [],
      "source": [
        "ntokens = len(token2ind) #fill me # the size of vocabulary\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrEFiUGjh8TR",
        "outputId": "9443a05d-ab00-4f2c-cb66-8829955d92cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50001"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u-OLy4KIkDwf"
      },
      "outputs": [],
      "source": [
        "# optimization paramerters\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bwh3n9xZQy4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8dc41f-0d6f-48df-84bd-67bc76c55de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-07 18:42:11--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10146460 (9.7M) [text/plain]\n",
            "Saving to: ‘pretraining_subset.txt’\n",
            "\n",
            "pretraining_subset. 100%[===================>]   9.68M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-11-07 18:42:12 (71.2 MB/s) - ‘pretraining_subset.txt’ saved [10146460/10146460]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "path_data_train = \"pretraining_subset.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0m11g4ScjZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757b0a4b-b85d-41bd-fa48-38ad44c3c976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 3125 steps | loss 7.59998 | ppl 1998.152\n",
            "| epoch   1 |  1000/ 3125 steps | loss 6.84606 | ppl  940.169\n",
            "| epoch   1 |  1500/ 3125 steps | loss 6.57510 | ppl  717.016\n",
            "| epoch   1 |  2000/ 3125 steps | loss 6.38353 | ppl  592.015\n",
            "| epoch   1 |  2500/ 3125 steps | loss 6.25064 | ppl  518.347\n",
            "| epoch   1 |  3000/ 3125 steps | loss 6.15043 | ppl  468.918\n",
            "| epoch   2 |   500/ 3125 steps | loss 5.86090 | ppl  351.041\n",
            "| epoch   2 |  1000/ 3125 steps | loss 5.81392 | ppl  334.931\n",
            "| epoch   2 |  1500/ 3125 steps | loss 5.74450 | ppl  312.468\n",
            "| epoch   2 |  2000/ 3125 steps | loss 5.73555 | ppl  309.683\n",
            "| epoch   2 |  2500/ 3125 steps | loss 5.65272 | ppl  285.067\n",
            "| epoch   2 |  3000/ 3125 steps | loss 5.65532 | ppl  285.808\n"
          ]
        }
      ],
      "source": [
        "#pretraining on a tiny subset\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        path_data_train,\n",
        "        save_interval=-1,\n",
        "        task= \"language_modeling\" , # fill me\n",
        "        batch_size=16,\n",
        "        log_interval=log_interval,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-BcBC6FSkMH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493a2095-c7d5-45ca-9c08-ea2f50152fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-07 18:47:12--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88093955 (84M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_model_4layers.pt’\n",
            "\n",
            "pretrained_model_4l 100%[===================>]  84.01M   344MB/s    in 0.2s    \n",
            "\n",
            "2022-11-07 18:47:19 (344 MB/s) - ‘pretrained_model_4layers.pt’ saved [88093955/88093955]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device) \n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt') \n",
        "#load state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tBRRVsWqlIoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6822a424-bf63-4f37-88b1-97340247ad09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 27.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "--2022-11-07 18:47:24--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115362 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘sentencepiece.french.model’\n",
            "\n",
            "sentencepiece.frenc 100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-11-07 18:47:25 (153 MB/s) - ‘sentencepiece.french.model’ saved [1115362/1115362]\n",
            "\n",
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece   # uncomment this if you are using google colab\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "outputs": [],
      "source": [
        "def infer_next_token(sent):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces] # list of tokens\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    next_token_ind = torch.argmax(out[-1,:,:]) #fill me\n",
        "    return next_token_ind, out\n",
        "    \n",
        "def infer_next_tokens(sent, max_len=50):\n",
        "    # to be implemented\n",
        "    for _ in range(max_len):\n",
        "      next_token_ind = infer_next_token(sent)[0]\n",
        "      sent = sent + \" \" + ind2token[next_token_ind.item()]\n",
        "      if next_token_ind.item() == 2:\n",
        "        return sent\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f83Nn5nSly4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4ea1f1bc-285b-4af3-a438-591238f301e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour les ▁gens ▁qui ▁ont ▁été ▁très ▁accueillants ▁et ▁sympathiques . <eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sent = \"Bonjour les\"\n",
        "infer_next_tokens(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0K1BZsblmEmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5096714-ceca-4a83-9abe-acf2f6b8b34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-07 18:47:25--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1495960 (1.4M) [text/plain]\n",
            "Saving to: ‘train.review.spm’\n",
            "\n",
            "train.review.spm    100%[===================>]   1.43M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-11-07 18:47:27 (195 MB/s) - ‘train.review.spm’ saved [1495960/1495960]\n",
            "\n",
            "--2022-11-07 18:47:27--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200 (3.1K) [text/plain]\n",
            "Saving to: ‘train.label’\n",
            "\n",
            "train.label         100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-07 18:47:29 (42.2 MB/s) - ‘train.label’ saved [3200/3200]\n",
            "\n",
            "--2022-11-07 18:47:29--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864544 (1.8M) [text/plain]\n",
            "Saving to: ‘test.review.spm’\n",
            "\n",
            "test.review.spm     100%[===================>]   1.78M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-11-07 18:47:31 (273 MB/s) - ‘test.review.spm’ saved [1864544/1864544]\n",
            "\n",
            "--2022-11-07 18:47:31--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4000 (3.9K) [text/plain]\n",
            "Saving to: ‘test.label’\n",
            "\n",
            "test.label          100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-07 18:47:32 (58.9 MB/s) - ‘test.label’ saved [4000/4000]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "\n",
        "path_data_train = \"train.review.spm\"\n",
        "path_labels_train = \"train.label\"\n",
        "\n",
        "path_data_valid = \"test.review.spm\"\n",
        "path_labels_valid = \"test.label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "outputs": [],
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader):\n",
        "    #to be implemented\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    ncorrect = ntotal = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(device)\n",
        "            input = data[0].to(device)\n",
        "            output = model(input, src_mask) \n",
        "            output = output[-1,:,:] \n",
        "\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "            target = data[1].to(device)\n",
        "            # total number of examples\n",
        "            ntotal +=  target.shape[0]\n",
        "            # number of correct predictions \n",
        "            predictions = torch.argmax(output, dim=1)\n",
        "            ncorrect += torch.sum(target == predictions) \n",
        "        acc = ncorrect.item() / ntotal\n",
        "        \n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "outputs": [],
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "i-xclMCpnVpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f929f598-219a-4ac1-cb4b-ea22041e18a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/  200 steps | loss 0.81749 | ppl    2.265\n",
            "| epoch   1 |   100/  200 steps | loss 0.79096 | ppl    2.206\n",
            "| epoch   1 |   150/  200 steps | loss 0.75183 | ppl    2.121\n",
            "| epoch   2 |    50/  200 steps | loss 0.75850 | ppl    2.135\n",
            "| epoch   2 |   100/  200 steps | loss 0.71384 | ppl    2.042\n",
            "| epoch   2 |   150/  200 steps | loss 0.73747 | ppl    2.091\n",
            "| epoch   3 |    50/  200 steps | loss 0.66772 | ppl    1.950\n",
            "| epoch   3 |   100/  200 steps | loss 0.64441 | ppl    1.905\n",
            "| epoch   3 |   150/  200 steps | loss 0.60378 | ppl    1.829\n",
            "| epoch   4 |    50/  200 steps | loss 0.45697 | ppl    1.579\n",
            "| epoch   4 |   100/  200 steps | loss 0.47514 | ppl    1.608\n",
            "| epoch   4 |   150/  200 steps | loss 0.39236 | ppl    1.480\n",
            "| epoch   5 |    50/  200 steps | loss 0.19336 | ppl    1.213\n",
            "| epoch   5 |   100/  200 steps | loss 0.21671 | ppl    1.242\n",
            "| epoch   5 |   150/  200 steps | loss 0.28404 | ppl    1.328\n",
            "| epoch   6 |    50/  200 steps | loss 0.08931 | ppl    1.093\n",
            "| epoch   6 |   100/  200 steps | loss 0.06599 | ppl    1.068\n",
            "| epoch   6 |   150/  200 steps | loss 0.06090 | ppl    1.063\n",
            "| epoch   7 |    50/  200 steps | loss 0.08105 | ppl    1.084\n",
            "| epoch   7 |   100/  200 steps | loss 0.03978 | ppl    1.041\n",
            "| epoch   7 |   150/  200 steps | loss 0.03691 | ppl    1.038\n",
            "| epoch   8 |    50/  200 steps | loss 0.01751 | ppl    1.018\n",
            "| epoch   8 |   100/  200 steps | loss 0.03259 | ppl    1.033\n",
            "| epoch   8 |   150/  200 steps | loss 0.03080 | ppl    1.031\n",
            "| epoch   9 |    50/  200 steps | loss 0.00788 | ppl    1.008\n",
            "| epoch   9 |   100/  200 steps | loss 0.01892 | ppl    1.019\n",
            "| epoch   9 |   150/  200 steps | loss 0.00079 | ppl    1.001\n",
            "| epoch  10 |    50/  200 steps | loss 0.00011 | ppl    1.000\n",
            "| epoch  10 |   100/  200 steps | loss 0.01468 | ppl    1.015\n",
            "| epoch  10 |   150/  200 steps | loss 0.00009 | ppl    1.000\n",
            "| epoch  11 |    50/  200 steps | loss 0.00019 | ppl    1.000\n",
            "| epoch  11 |   100/  200 steps | loss 0.03129 | ppl    1.032\n",
            "| epoch  11 |   150/  200 steps | loss 0.00027 | ppl    1.000\n",
            "| epoch  12 |    50/  200 steps | loss 0.00358 | ppl    1.004\n",
            "| epoch  12 |   100/  200 steps | loss 0.00004 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.01948 | ppl    1.020\n",
            "| epoch  13 |    50/  200 steps | loss 0.02217 | ppl    1.022\n",
            "| epoch  13 |   100/  200 steps | loss 0.00856 | ppl    1.009\n",
            "| epoch  13 |   150/  200 steps | loss 0.01996 | ppl    1.020\n",
            "| epoch  14 |    50/  200 steps | loss 0.01113 | ppl    1.011\n",
            "| epoch  14 |   100/  200 steps | loss 0.02733 | ppl    1.028\n",
            "| epoch  14 |   150/  200 steps | loss 0.02457 | ppl    1.025\n",
            "| epoch  15 |    50/  200 steps | loss 0.01539 | ppl    1.016\n",
            "| epoch  15 |   100/  200 steps | loss 0.01615 | ppl    1.016\n",
            "| epoch  15 |   150/  200 steps | loss 0.01661 | ppl    1.017\n",
            "\n",
            "=====PRETRAINED MODEL======\n",
            "| epoch   1 |    50/  200 steps | loss 0.91388 | ppl    2.494\n",
            "| epoch   1 |   100/  200 steps | loss 0.80279 | ppl    2.232\n",
            "| epoch   1 |   150/  200 steps | loss 0.76208 | ppl    2.143\n",
            "| epoch   2 |    50/  200 steps | loss 0.70958 | ppl    2.033\n",
            "| epoch   2 |   100/  200 steps | loss 0.61645 | ppl    1.852\n",
            "| epoch   2 |   150/  200 steps | loss 0.55801 | ppl    1.747\n",
            "| epoch   3 |    50/  200 steps | loss 0.51654 | ppl    1.676\n",
            "| epoch   3 |   100/  200 steps | loss 0.49946 | ppl    1.648\n",
            "| epoch   3 |   150/  200 steps | loss 0.45149 | ppl    1.571\n",
            "| epoch   4 |    50/  200 steps | loss 0.41133 | ppl    1.509\n",
            "| epoch   4 |   100/  200 steps | loss 0.42775 | ppl    1.534\n",
            "| epoch   4 |   150/  200 steps | loss 0.42445 | ppl    1.529\n",
            "| epoch   5 |    50/  200 steps | loss 0.31422 | ppl    1.369\n",
            "| epoch   5 |   100/  200 steps | loss 0.37380 | ppl    1.453\n",
            "| epoch   5 |   150/  200 steps | loss 0.35257 | ppl    1.423\n",
            "| epoch   6 |    50/  200 steps | loss 0.29117 | ppl    1.338\n",
            "| epoch   6 |   100/  200 steps | loss 0.33027 | ppl    1.391\n",
            "| epoch   6 |   150/  200 steps | loss 0.35330 | ppl    1.424\n",
            "| epoch   7 |    50/  200 steps | loss 0.23434 | ppl    1.264\n",
            "| epoch   7 |   100/  200 steps | loss 0.27986 | ppl    1.323\n",
            "| epoch   7 |   150/  200 steps | loss 0.26219 | ppl    1.300\n",
            "| epoch   8 |    50/  200 steps | loss 0.16094 | ppl    1.175\n",
            "| epoch   8 |   100/  200 steps | loss 0.26698 | ppl    1.306\n",
            "| epoch   8 |   150/  200 steps | loss 0.21694 | ppl    1.242\n",
            "| epoch   9 |    50/  200 steps | loss 0.13978 | ppl    1.150\n",
            "| epoch   9 |   100/  200 steps | loss 0.18306 | ppl    1.201\n",
            "| epoch   9 |   150/  200 steps | loss 0.16803 | ppl    1.183\n",
            "| epoch  10 |    50/  200 steps | loss 0.12557 | ppl    1.134\n",
            "| epoch  10 |   100/  200 steps | loss 0.19507 | ppl    1.215\n",
            "| epoch  10 |   150/  200 steps | loss 0.13319 | ppl    1.142\n",
            "| epoch  11 |    50/  200 steps | loss 0.11662 | ppl    1.124\n",
            "| epoch  11 |   100/  200 steps | loss 0.07894 | ppl    1.082\n",
            "| epoch  11 |   150/  200 steps | loss 0.15697 | ppl    1.170\n",
            "| epoch  12 |    50/  200 steps | loss 0.02377 | ppl    1.024\n",
            "| epoch  12 |   100/  200 steps | loss 0.12816 | ppl    1.137\n",
            "| epoch  12 |   150/  200 steps | loss 0.08217 | ppl    1.086\n",
            "| epoch  13 |    50/  200 steps | loss 0.02654 | ppl    1.027\n",
            "| epoch  13 |   100/  200 steps | loss 0.06736 | ppl    1.070\n",
            "| epoch  13 |   150/  200 steps | loss 0.09064 | ppl    1.095\n",
            "| epoch  14 |    50/  200 steps | loss 0.02469 | ppl    1.025\n",
            "| epoch  14 |   100/  200 steps | loss 0.08257 | ppl    1.086\n",
            "| epoch  14 |   150/  200 steps | loss 0.12137 | ppl    1.129\n",
            "| epoch  15 |    50/  200 steps | loss 0.06252 | ppl    1.065\n",
            "| epoch  15 |   100/  200 steps | loss 0.06483 | ppl    1.067\n",
            "| epoch  15 |   150/  200 steps | loss 0.05948 | ppl    1.061\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from_scratch_settings = [True, False]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            path_data_train,\n",
        "            path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task='classification',\n",
        "            batch_size=8,\n",
        "            log_interval=50,\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            get_loader(\n",
        "                path_data_valid,\n",
        "                path_labels_valid,\n",
        "                token2ind=token2ind,\n",
        "                batch_size=20,\n",
        "                task='classification',\n",
        "            )\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RCpBIdTHojm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "1c9103c8-9499-4a84-ba82-303bfad7dad8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Visualize the accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(from_scratch_valid_acc, label=\"From scratch\")\n",
        "plt.plot(pretrained_valid_acc, label=\"pre-trained\")\n",
        "plt.xlabel(\"#epochs\")\n",
        "plt.ylabel(\"validation accuracy\")\n",
        "# plt.ylim(0,1)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZBxL0b6Y55w"
      },
      "execution_count": 21,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "1f3cfdeab8dd8f9900bd16266619de191cf0f5e09365d74b1fba1714dce58066"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}