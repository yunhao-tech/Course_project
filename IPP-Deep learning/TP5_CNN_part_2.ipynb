{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KgrfZuOKA6r"
      },
      "source": [
        "# TP Coding Convolutional Neural Networks in Pytorch - part 2\n",
        "\n",
        "Author : Alasdair Newson\n",
        "\n",
        "alasdair.newson@telecom-paris.fr\n",
        "\n",
        "In this session, we shall be looking at two subjects :\n",
        "\n",
        "- A way to visualise what networks are learning : the Deep Dream algorithm\n",
        "- Adversarial examples\n",
        "\n",
        "At the heart of these two applications is the calculation of the gradient of a loss function with respect to the image itself (instead of respect to the weights). The loss function will be defined depending on the application at hand.\n",
        "\n",
        "We shall use a pre-trained, well-known pretrained network. The lab has been designed for the VGG19 architecture. You can try with other networks, but bear in mind that the pre-processing operations are different for each network, so you would have to change the code correspondingly.\n",
        "\n",
        "First, let's load the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtjQ-Fl4KA6s"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image, ImageFilter, ImageChops\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "using_colab=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49mhdiz8KA6x"
      },
      "source": [
        "Now, let's load one of the most famous networks, VGG16 or inceptionv3 (you choose), and view it's architecture with the eval() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlxxtnk_KA6x"
      },
      "source": [
        "\n",
        "model_name = 'vgg19'\n",
        "\n",
        "vgg_model = models.vgg19(pretrained=True)\n",
        "vgg_model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEC3QZbuKA7F"
      },
      "source": [
        "You can visualise the weights if you like in a similar manner to the first part of the TP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue4N3je6KA7Q"
      },
      "source": [
        "## 2. Deep Dream\n",
        "\n",
        "We now proceed to carry out the Deep Dream algorithm. The idea of the Deep Dream algorithm is to find an image which maximises the response of a network at a certain layer : $\\textbf{this should help us understand what the network is learning}$. This can be done with an iterative algorithm, by simply carrying out gradient $\\textbf{ascension}$. We start with an input image and iteratively add the gradient of the average response of the features which interest us. A pseudo-code for this would be :\n",
        "\n",
        "- img = img_in\n",
        "- for i=1:n_iters\n",
        "    - img = img + grad_step $\\nabla_{img} \\mathcal{L}$,\n",
        "    \n",
        "where $\\mathcal{L}$ is a function of the responses which interest us (you need to define this). In this part of the lab, we shall use the average squared response.\n",
        "\n",
        "Let's first define a function to preprocess the image. This is needed to put the image in the correct format for the VGG19 network. We also create a function to invert this process. You can add cases for other networks if you wish (and you can get it working :) )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib1WJOo5KA7R"
      },
      "source": [
        "normalise_resize = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalisation\n",
        "])\n",
        "\n",
        "def format_image(img_file):\n",
        "\t\"\"\"\n",
        "\tThis function reads and formats an image so that it can be fed to the VGG16 networks\n",
        "\t\n",
        "\tParameters\n",
        "\t----------\n",
        "\timg_file : image file name\n",
        "\t\n",
        "\tReturns\n",
        "\t-------\n",
        "\timg_out_model : the correctly formatted image for VGG16 or inceptionv3 networks\n",
        "\timg : the image as read by the load_img function of keras.preprocessing.image\n",
        "\t\"\"\"\n",
        "\t# read image\n",
        "\timg = PIL.Image.open(img_file)\n",
        "\timg_tensor = normalise_resize(img).unsqueeze(axis=0)\n",
        "\timg_np = np.array(img)\n",
        " \n",
        "\treturn img_tensor, img_np\n",
        "\n",
        "def unformat_image(img_in):\n",
        "\t\"\"\"\n",
        "\tThis function inverts the preprocessing applied to images for use in the VGG16/inceptionv3 network\n",
        "\t\n",
        "\tParameters\n",
        "\t----------\n",
        "\timg_file : formatted image of shape (batch_size,m,n,3)\n",
        "\t\n",
        "\tReturns\n",
        "\t-------\n",
        "\timg_out : a m-by-n-by-3 array, representing an image that can be written to an image file\n",
        "\t\"\"\"\n",
        "\timg_in = np.transpose(img_in.numpy().squeeze(),[1,2,0])\n",
        "\t# invert the mean and standard deviation\n",
        "\tmean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n",
        "\tstd = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n",
        "\timg_out = std * img_in + mean\n",
        "\timg_out *= 255\n",
        "\timg_out = np.uint8(np.clip(img_out, 0, 255))\n",
        "\treturn img_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "570yJTBzKA7U"
      },
      "source": [
        "Now, we load the image. ```format_image``` outputs two variables: the Pytorch tensor containing the image information, and a normal numpy array with this information, to visualise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz37JIMqKA7V"
      },
      "source": [
        "\n",
        "#load image\n",
        "if (using_colab == True):\n",
        "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg\"\n",
        "    img_in,img_np = format_image('got.jpg')\n",
        "else:\n",
        "    img_in,img_np = format_image('images/got.jpg')\n",
        "#show the input image\n",
        "plt.imshow(img_np)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: we have resized the image to be accepted as an input by VGG19. Strictly speaking, we do not actually have to do this, since we are only using the _convolutional_ part of VGG19 (and not the classification layer), but since we will be using the full network afterwards, we may as well take care of this resizing problem now. This is the resized image: "
      ],
      "metadata": {
        "id": "m34XAb7_gIsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(unformat_image(img_in))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JrCPj9j8PQfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykQiJhGyKA7Y"
      },
      "source": [
        "## Deep dream gradient ascension\n",
        "\n",
        "We will implement the following elements in a function named ```deep_dream_gradient_step```.\n",
        "\n",
        "### Loss function (loss to _maximise_)\n",
        "We need to define the loss that we wish to maximise. This can be anything you wish, but a common loss is simply the average squared response of certain layers. So, we loop over the layers of the network, and extract the responses at the layers which interest us. We take the average value of the squared responses as our loss.\n",
        "\n",
        "### 2/ Gradient step\n",
        "\n",
        "Next, we need to carry out one gradient step of the deep dream algorithm. To do this, we need the gradients of the loss with respect to the image. Pytorch allows one to do this quite easily.\n",
        "\n",
        "Thus, _before_ defining the loop over the layers, we first create a Pytorch ```Variable``` from the input image, to which we can backpropagate the loss's gradient. This is done  with:\n",
        "\n",
        "- ```y = Variable(x, requires_grad=True)```\n",
        "\n",
        "The gradient is backpropagated to all elements contributing to this loss with:\n",
        "\n",
        "- ```loss.backward()```\n",
        "\n",
        "We can then extract the gradients from ```y``` with:\n",
        "\n",
        "- ```y.grad.data```\n",
        "\n",
        "Finally, we use a little trick. Indeed, it may be the case that the gradients are far too small or far too large for updating, meaning that the updates do nothing or destroy the image. To avoid this, we normalise the gradients, such that the standard deviation is 1.\n",
        "\n",
        "- $\\nabla = \\frac{\\nabla}{std(\\nabla) + \\epsilon}$\n",
        "\n",
        "We also add a small constant $ɛ$ to avoid division by 0. Set this constant to 1e-8.\n",
        "\n",
        "Fill in the following function which carries out a step of the gradient ascent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52qz7Kh2-TUW"
      },
      "source": [
        "\n",
        "\n",
        "def deep_dream_gradient_step(img_in, model_in, layer_ids_to_use, step_size):\n",
        "\n",
        "\tmean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n",
        "\tstd = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n",
        "\n",
        "\tactivations = []\n",
        "\tlosses = []\n",
        "\timg_var = Variable(img_in, requires_grad=True) # FILL in student\n",
        "\tx = img_var\n",
        "\t# First, we fetch the activations/feature maps which we want to analyse, when applied to img_in\n",
        "\tfor index, layer in enumerate(vgg_model.features):\n",
        "\t\t# apply the layers iteratively\n",
        "\t\tx = ...  # FILL in student\n",
        "\t\tif index in set(layer_inds):\n",
        "\t\t\tactivations.append(x) \n",
        "\t \n",
        "\t# now loop over the layers which interest you and calculate the average squared response, and add this to the list of losses.\n",
        "\tfor layer_activation in activations:\n",
        "\t\tloss_component = ... # FILL IN STUDENT\n",
        "\t\tlosses.append(loss_component)\n",
        "\n",
        "\tfinal_loss = torch.mean(torch.stack(losses))\n",
        "\t# carry out backpropagation\n",
        "\t...\n",
        "\n",
        "\t# get gradients of loss function with respect to the image\n",
        "\tgrad = ... # FILL IN STUDENT\n",
        "\n",
        "\t# Normalize the gradients (make them have mean = 0 and std = 1)\n",
        "\tg_std = torch.std(grad)\n",
        "\tg_mean = torch.mean(grad)\n",
        "\tgrad = ...   # FILL IN STUDENT\n",
        "\tgrad = ...  # FILL IN STUDENT\n",
        "\n",
        "\t# Update image using the calculated gradients (gradient ascent step)\n",
        "\timg_in.data += ...   # FILL IN STUDENT\n",
        "\n",
        "\t# We clamp the data to avoid going too far from the VGG normalisation\n",
        "\timg_in[0, :, :, :] = np.clip(img_in[0, :, :, :], -mean / std, (1 - mean) / std)\n",
        " \n",
        "\treturn img_in, final_loss\n",
        "\t#END STUDENT CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK1SmII3KA7o"
      },
      "source": [
        "We are now ready to carry out the Deep Dream algorithm using gradient ascent, yipee ! Iterate 'n_iterations' times, each time adding an epsilon of the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp7CApEuKA7o"
      },
      "source": [
        "layer_inds = [21,22]   # use these layers as default\n",
        "\n",
        "# first, reload image to make sure that we are not starting from a previous initialisation\n",
        "if (using_colab == True):\n",
        "\t#!wget 'https://perso.telecom-paristech.fr/anewson/doc/images/got.jpg'\n",
        "\timg_in,_ = format_image('got.jpg')\n",
        "else:\n",
        "\timg_in,_ = format_image('images/got.jpg')\n",
        "\n",
        "step_size = 0.09 # Gradient ascent step size\n",
        "n_iterations = 30  # Number of gradient ascent steps\n",
        "\n",
        "img_out = unformat_image(torch.clone(img_in))\n",
        "plt.figure(num=None, figsize=(10, 8))\n",
        "plt.imshow(img_out)\n",
        "plt.show()\n",
        "\n",
        "for ii in range(0,n_iterations):\n",
        "\timg_in, loss = deep_dream_gradient_step(...) # FILL IN STUDENT\n",
        "\t\n",
        "\tif (ii%5==0):\n",
        "\t\tprint(\".\", end='')\n",
        "\t\timg_out = unformat_image(torch.clone(img_in))\n",
        "\t\tplt.figure(num=None, figsize=(10, 8))\n",
        "\t\tplt.imshow(img_out)\n",
        "\t\tplt.show()\n",
        "    #plt.imsave('img_out_'+str(ii).zfill(3)+'.png',img_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPemFEHKA7s"
      },
      "source": [
        "You can try different layers and see what the results are !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52DV8fzHKA7s"
      },
      "source": [
        "## 3. Adversarial examples\n",
        "\n",
        "In this part of the lab work, we will explore the interesting case of adversarial examples. Adversarial examples are images which have been perturbed in a manner which __makes the network misclassify the image__.\n",
        "\n",
        "There are many ways to do this, however we can use a similar approach to the one used above, that is to say, we will use a gradient maximisation approach. This consists in iteratively adding the gradient of a redefined loss with respect to the image, to the current image, in order to get a misclassified image.\n",
        "\n",
        "This loss will simply be the element (class) of the output (distribution) of the network which we want to maximise.\n",
        "\n",
        "First, let's reload another image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkGPxU8JKA70"
      },
      "source": [
        "if (using_colab == True):\n",
        "    !wget \"https://perso.telecom-paristech.fr/anewson/doc/images/cat_small.png\"\n",
        "    img_in,img_visu = format_image('cat_small.png')\n",
        "else:\n",
        "    img_in,img_visu = format_image('images/cat_small.png')\n",
        "\n",
        "plt.imshow(img_visu)\n",
        "print(img_in.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look at the VGG19 architecture, we se that the output is a ```linear``` layer, so there is no non-linearity (although there should be a softmax). This is because Pytorch automatically carries out the softmax during training (we saw this in part 1).\n",
        "\n",
        "Therefore, define again (as in part 1) a function called ```vector_to_class``` which converts a vector to a class."
      ],
      "metadata": {
        "id": "HD2opKE7lg7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(torch.nn.Softmax()(x),axis=1)  # FILL IN STUDENT\n",
        "  return y"
      ],
      "metadata": {
        "id": "jsZcsOi9CHyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRayrKxmKA7-"
      },
      "source": [
        "We are going to try to force the image to recognise a 'reflex_camera'. This is number 759 of the imagenet classes. You can use any one you like in fact (apart from ones linked to cats, obviously). To see the list of classes go to :\n",
        "\n",
        "https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "\n",
        "First, we find the initial class of the cat image (which we consider to be the true class). Here, we have downloaded the imagenet label list to convert a class number to a string (name of the class)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D23TXNTtKA7_"
      },
      "source": [
        "# get the image net class names\n",
        "!wget \"https://perso.telecom-paristech.fr/anewson/doc/data/image_net/imagenet1000_clsidx_to_labels.txt\"\n",
        "imagenet_labels = np.array(open(\"imagenet1000_clsidx_to_labels.txt\").read().splitlines())\n",
        "\n",
        "target_class = 759\n",
        "# carry out the network predictions on the example image\n",
        "#define the true class as the initial most likely class\n",
        "true_class = ... # FILL IN STUDENT CODE\n",
        "true_class_name = imagenet_labels[true_class]\n",
        "print(\"True class : \", true_class_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkP6U09oKA8D"
      },
      "source": [
        "We must redefine the gradient step function for finding the adversarial example. It now takes the ```target_class``` instead of the list of layer indices to maximise. Therefore, note that you do not have to loop over all the layers, but simply take the output of the network.\n",
        "\n",
        "Fill in the following function accordingly (you can use much of the previous code).\n",
        "\n",
        "__IMPORTANT NOTE !!!__ : Remember that the output of the VGG (in Pytorch) is a linear layer, so a vector which is _not_ normalised to be a probability distribution. Therefore, you need to apply a ```torch.nn.Softmax()(.)``` to the output before calculating the loss (otherwise you will increase the target class without decreasing the other classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eawea0e4QWCx"
      },
      "source": [
        "def adversarial_example_gradient_step(img_in, model_in, target_class, step_size):\n",
        "\n",
        "  # BEGIN STUDENT CODE\n",
        "\n",
        "  # END STUDENT CODE\n",
        "  return img_in, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZj67FUeR-_a"
      },
      "source": [
        "We are now ready to perturb the image such that we misclassify it. Youhoo !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y89wcKb1KA8K",
        "scrolled": true
      },
      "source": [
        "\n",
        "# first, reload image to make sure that we are not starting from a previous initialisation\n",
        "if (using_colab == True):\n",
        "\timg_in,_ = format_image('cat_small.png')\n",
        "else:\n",
        "\timg_in,_ = format_image('images/cat_small.png')\n",
        "\n",
        "step_size = 0.01 # Gradient ascent step size\n",
        "n_iterations = 40  # Number of gradient ascent steps\n",
        "\n",
        "current_class = int(vector_to_class(vgg_model(img_in))) # FILL IN STUDENT\n",
        "current_class_name = imagenet_labels[current_class]\n",
        "\n",
        "plt.figure(num=None, figsize=(10, 8))\n",
        "img_out = unformat_image(torch.clone(img_in))\n",
        "plt.imshow(img_out)\n",
        "plt.show()\n",
        "print(\"Current class : {} with probability : {:.4f}\".format(current_class_name, torch.nn.Softmax()(vgg_model(img_in)[0,current_class])))\n",
        "print(\".\", end='')\n",
        "\n",
        "for ii in range(0,n_iterations):\n",
        "  img_in, loss = adversarial_example_gradient_step(...) # FILL IN STUDENT\n",
        "  if (ii%1==0):\n",
        "    #get the current class\n",
        "    current_class = int(vector_to_class(vgg_model(img_in))) # FILL IN STUDENT\n",
        "    current_class_name = imagenet_labels[current_class]\n",
        "    plt.figure(num=None, figsize=(10, 8))\n",
        "    img_out = unformat_image(torch.clone(img_in))\n",
        "    plt.imshow(img_out)\n",
        "    plt.show()\n",
        "    print(\"Current class : {} with probability : {:.4f}\".format(current_class_name, torch.nn.Softmax()(vgg_model(img_in)[0,current_class])))\n",
        "    print(\".\", end='')\n",
        "    #plt.imsave('img_out_'+str(ii).zfill(3)+'.png',img_out)\n",
        "\n",
        "img_out = unformat_image(torch.clone(img_in))\n",
        "plt.figure(num=None, figsize=(10, 8))\n",
        "plt.imshow(img_out)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uObcnTqBKA8N"
      },
      "source": [
        "As you should probably see, the image is changed such that it is no longer is correctly classified. It should be classified as a 'reflex camera', or whatever you chose, with high probability. This is a problem, since a human is still able to see a cat !"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the work, you should rate the code for :\n",
        "- 1) Deep dream, ```retrieve_weights``` function : 2 points\n",
        "- 2) Deep dream, ```deep_dream_loss``` function : 1 point\n",
        "- 3) Deep dream, ```gradient_step``` function : 2 points\n",
        "- 4) Adversarial examples, choosing the right layer for the loss : 1 point\n",
        "- 5) Adversarial examples, ```y_predicted```, ```true_class``` and ```y_predicted_decoded``` : 3 Points\n",
        "- 6) Adversarial examples,  ```adversarial_loss``` function : 1 point\n",
        "\n",
        "Total over 10 points."
      ],
      "metadata": {
        "id": "THK-7_-UFrIV"
      }
    }
  ]
}