---
title: " Graph Clustering: Spectral and hierarchical methods"
subtitle: "correction"
format:
  html:
    self-contained: true
    theme: [cosmo, theme.scss]
    toc: true
    number-sections: true
    html-math-method: katex
    code-copy: true
    code-summary: "Show the code"
    code-overflow: wrap
---

##  Introduction

This tutorial introduces the graph clustering techniques seen during the lectures, that is,

- Hierarchical clustering for network data
- Spectral clustering and its variants

These methods will be illustrated for the analysis of a (friendship/social) network data set.

###  Requirements

The packages required for the analysis are `igraph`,  `igraphdata`, `aricode` plus some others for data manipulation and representation. 

```{r packages, message=FALSE, warning=FALSE}
library(igraph)     # graph manipulation
library(igraphdata) # network data manipulation
library(tidyverse)  # data manipulation
library(corrplot)   # fancy matrix representation
library(aricode)    # clustering measures comparison
```

::: {.callout-info}
## Advice

*Use the documentation of the aforementioned packages!!*

:::

### Data set: Friendship network of a UK university faculty

The personal friendship network of a faculty of a UK university, consists of 81 vertices (individuals) and 817 directed and weighted connections. The school affiliation of each individual is stored as a vertex attribute. This dataset can serve as a tested for community detection algorithms.

We consider a undirected version of this network, available with the package **igraphdata**:

```{r import dataset}
data("UKfaculty", package = "igraphdata")
UKfaculty <- as.undirected(UKfaculty)
UKfaculty
```

This data set is an igraph object with various attributes

- `V(UKfaculty)$Group`  : a node attribute for the school affiliation
- `E(UKfaculty)$weight` : an edge attribute for the weights / freindship strength 

## Analysis of the Friendship network

### Binary interaction network

We first consider a binary version of this network where an edge is drawn between two individual when they are friend, just to become familiar with **igraph**.

#### Questions {-}

1. Create an igraph object similar to `UKfaculty` with binary edges

```{r}
UKbinary <- UKfaculty
n_edges <- gsize(UKfaculty)
E(UKbinary)$weight <- rep(1, n_edges)
```

2. Check the attributes of the vertices and edges. Plot the network by adding color to the node related to the group.

```{r}
affiliation <- V(UKfaculty)$Group
plot.igraph(UKbinary, vertex.color = affiliation)
summary(UKbinary)
```

3. Extract the binary adjacency matrix and plot it with the **corrplot** package. 

```{r}
Y <- UKbinary %>% as_adjacency_matrix() %>% as.matrix()
o <- order(affiliation)
Y[o,o] %>% corrplot(is.corr = FALSE, tl.pos = 'n', cl.pos = 'n') # cluster it
```

3. Plot the distribution of the degree

```{r}
hist(degree(UKbinary))
```


4. Plot the matrix of shortest-path distance

```{r}
D_shortest <- distances(UKbinary)
corrplot(D_shortest, is.corr = FALSE, tl.pos = 'n')
corrplot(D_shortest[o,o], is.corr = FALSE, tl.pos = 'n') # cluster according to affiliation
```

5. Perform hierarchical clustering using various algorithm (modularity, edges betweeness, etc). What do you think?

Let us first consider the algorithms for community detection already implemented in **igraph**:

```{r}
hc_path <- hclust(as.dist(D_shortest), method = "ward.D2")
plot(hc_path)
shortest_path_cl <- cutree(hc_path, 4)
table(affiliation, shortest_path_cl)
ARI(affiliation, shortest_path_cl) # the similarity between the theoretical clustering and prediction clustering. 
                                  # ARI = 1 means a perfect matching.
```

```{r, warning=FALSE}
hc_modularity  <- igraph::cluster_fast_greedy(UKbinary)
hc_betweenness <- igraph::cluster_edge_betweenness(UKbinary)
hc_louvain     <- igraph::cluster_louvain(UKbinary)
plot(hc_modularity, UKbinary)
plot(hc_betweenness, UKbinary)
plot(hc_louvain, UKbinary)


modularity_cl <- cutree(as.hclust(hc_modularity), 4)
table(affiliation, modularity_cl)
ARI(affiliation, modularity_cl)

```

Work pretty well!

### Weighted interaction network

1. We now study the original weighted graph. Plot the graph, the matrix and the distribution of the degree. Put color on node depending on the group attribute, node size depending on the node degree, and edge thickness depending on the weights.

```{r}
Y_weighted <- UKfaculty %>% as_adj(attr= "weight") %>% as.matrix() # specify to keep the weights
Y_weighted[o,o] %>% corrplot(is.corr = FALSE, tl.pos = 'n', cl.pos = 'n')

plot(UKfaculty, 
     vertex.color = affiliation,
     vertex.size = degree(UKfaculty),
     edge.width = .1 *E(UKfaculty)$weight)

hist(degree(UKfaculty))
```

2. Perform hierarchical clustering using various distances, in particular the shortest path distance. Plot the dendrogram (use the as.hclust function if necessary)

```{r}
hc_modularity <- as.hclust(igraph::cluster_fast_greedy(UKfaculty))
hc_short_path <- hclust(as.dist(igraph::distances(UKfaculty)), method = "ward.D2")
plot(hc_modularity)
plot(hc_short_path)
```

3. Compare the AHC clustering to the reference clustering with ARI (package `aricode`) for a varying number of groups. 

```{r}
nb_cluster <- 1:10
perf <- data.frame(
  nb_cluster = nb_cluster,
  modularity    = apply(cutree(hc_modularity , nb_cluster), 2, ARI, affiliation),
  shortest_path = apply(cutree(hc_short_path , nb_cluster), 2, ARI, affiliation))

perf %>%  
  pivot_longer(-nb_cluster, names_to = "method", values_to = "ARI") %>% 
  group_by(method) %>% 
  ggplot() + aes(x = nb_cluster, y = ARI, color = method) + geom_line() +
  theme_bw()
```

4. Plot the Fiedler vector for this data (use normalized Laplacian). Comment (use tree names no find some structure in the data).

```{r}
L <- laplacian_matrix(UKfaculty, normalized = TRUE)
spec_L <- eigen(L)
practical_zero <- 1e-12
lambda  <- min(spec_L$values[spec_L$values>practical_zero])
fiedler <- spec_L$vectors[, which(spec_L$values == lambda)]

qplot(y = fiedler, color = factor(affiliation)) + theme_bw()
```

5. Implement different the normalized and absolute spectral clusterings and test them on this data. Plot the data matrix reordered by row and column according to these clustering.

```{r}
spectral_clustering <- function(graph, nb_cluster, normalized = TRUE) {
  L <- laplacian_matrix(graph, normalized = normalized)
  selected <- rev(1:ncol(L))[1:nb_cluster]
  U <- eigen(L)$vectors[, selected, drop = FALSE] # spectral decomposition
  if (nb_cluster > 1) U <- sweep(U, 1, sqrt(rowSums(U^2)), '/')
  U[is.na(U)] <- 0
  res <- kmeans(U, nb_cluster, nstart = 40) # number of iterations in k-means
  res
}

absolute_spectral_clustering <- function(graph, nb_cluster) {
  L <- laplacian_matrix(graph, normalized = TRUE)
  AL <- diag(1, ncol(L), ncol(L)) - L
  spec_AL <- eigen(AL)
  selected <- order(abs(spec_AL$values), decreasing = TRUE)[1:nb_cluster]
  U <- spec_AL$vectors[, selected, drop = FALSE]
  if (nb_cluster > 1) U <- sweep(U, 1, sqrt(rowSums(U^2)), '/')
  U[is.na(U)] <- 0
  res <- kmeans(U, nb_cluster, nstart = 40)
  res
}
```

```{r}
nb_cluster <- 1:10
sp_norm   <- map(nb_cluster, ~spectral_clustering(UKfaculty, .))
sp_abs    <- map(nb_cluster, ~absolute_spectral_clustering(UKfaculty, .))
```

```{r}
A <- UKfaculty %>%  as_adj(attr= "weight") %>% as.matrix()
par(mf.row = c(1,3))
corrplot(A[order(affiliation), order(affiliation)], title = "reordering: affilation", is.corr=FALSE, tl.pos = 'n')
corrplot(A[order(sp_norm[[4]]$cl), order(sp_norm[[4]]$cl)], title = "reordering: normalized spectral", is.corr=FALSE)
corrplot(A[order(sp_abs[[4]]$cl), order(sp_abs[[4]]$cl)], title = "reordering: absolute spectral", is.corr=FALSE)
```

6. Compare all the methods with ARI/NID (package `aricode`) to the reference clsutering for various number of groups. 

```{r}
perf$spectral_normalized <- map_dbl(sp_norm, function(sp) ARI(sp$cl, affiliation))  
perf$spectral_absolute   <- map_dbl(sp_abs , function(sp) ARI(sp$cl, affiliation))  
perf %>%  
  pivot_longer(-nb_cluster, names_to = "method", values_to = "ARI") %>% 
  group_by(method) %>% 
  ggplot() + aes(x = nb_cluster, y = ARI, color = method) + geom_line() +
  theme_bw()
```


<!-- 7. To select the best clustering in the spectral clustering method, we can try to construct an approximate BIC for the k-means clustering. To this end, assume that the k-means can be seen as a Gaussian mixture models with $K$ spherical component, that is  -->

<!-- $$ -->
<!-- Y_i | i \in k \sim \mathcal{N}(\mu_k, \sigma_k \mathbf{I}_p) -->
<!-- $$ -->
<!-- For $\theta = \{\mu_1,\dots,\mu_K,\sigma_1,\sigma_K \} $, write the corresponding BIC criteria with $\log \ell(\hat{\theta}; Y, \hat{Z})$. Use the output of the `$tot.withinss` and `$size` of the `kmeans` to estimate the $\sigma_k^2$. What model is chosen by the BIC spectral clustering ? -->

<!-- We have -->
<!-- $$ -->
<!-- <!-- \log \ell(\theta; Y, \hat{Z}) =  \sum_{k}\sum_{i \in k} \left(-\frac12\log(\sigma_k^2) - \frac{1}{2 \sigma_k^2} \|Y_i - \mu_k \|^2 \right) --> -->
<!-- <!-- $$ --> -->
<!-- <!-- from which we get  --> -->

<!-- <!-- $$ --> -->
<!-- <!-- \hat{\sigma}_k^2 = \frac{1}{n_k}\|Y_i - \mu_k \|^2 --> -->
<!-- <!-- $$ --> -->
<!-- <!-- and  --> -->

<!-- <!-- $$ --> -->
<!-- <!-- \begin{aligned} --> -->
<!-- <!-- BIC(k) & = -2 \log \ell(\hat{\theta}) + \log(n) \# \text{param} \\ --> -->
<!-- <!-- & = \sum_k n_k \log (\hat{\sigma}_k^2) + 2 \log(n) K + \text{cst}\\  --> -->
<!-- <!-- \end{aligned} --> -->
<!-- <!-- $$ --> -->

<!-- ```{r} -->
<!-- BIC_kmeans <- function(sp) { -->
<!--   nk <- sp$size -->
<!--   n  <- sum(nk) -->
<!--   K  <- length(sp$size) -->
<!--   wss <- sp$withinss -->
<!--   sigma2_hat <- wss / (nk -1) -->
<!--   bic <- sum(nk * log(sigma2_hat)) + 2 * K * log(n) -->
<!-- } -->
<!-- BIC_sp_abs  <- map_dbl(sp_abs , BIC_kmeans) -->
<!-- BIC_sp_norm <- map_dbl(sp_norm, BIC_kmeans) -->
<!-- plot(BIC_sp_norm) -->
<!-- plot(BIC_sp_abs) -->
<!-- WSS_sp_abs  <- map_dbl(sp_abs , 'tot.withinss') -->
<!-- WSS_sp_norm <- map_dbl(sp_norm, 'tot.withinss') -->
<!-- plot(WSS_sp_abs) -->
<!-- plot(WSS_sp_norm) -->

<!-- BIC_sp_norm -->
<!-- BIC_sp_abs -->
<!-- ``` -->

7. What is your best final model?

The spectral clustering methods seem to perform the best here. 
