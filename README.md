# Courses homeworks & projects

| Course       | Homeworks & Projects | Content | 
| :--:         | :--:                 | :--:    | 
| NLP (ENSAE) | [Project and paper](https://github.com/yunhao-tech/NLP_ENSAE_2023) | Intent classification in Sequential labelling tasks, using contextual embeddings | 
| Advanced Machine learning (ENSAE) | [Final project](Advanced%20Machine%20learning) | Understand and Fine-tune the ViT-Base/32 CLIP model | 
| Data camp (IPP) | [Data challenge](https://github.com/yunhao-tech/solar_wind) | Solar wind classification based on data measured by in-situ spacecraft | 
|  | [Design of Data challenge](https://github.com/yunhao-tech/precipitation-forecast) | Precipitation forecast: Based on 18 consecutive satellite radar frames, to predict the next 18 frames | 
| Deep learning II (IPP) | [Final project: RBM&DBN](https://github.com/yunhao-tech/RBM_DBN/tree/master) | Implement and train RBM (Restricted Boltzmann machine) and DBN (Deep belief network) from scratch | 
| Altegrad (MVA) | [Lab1](AlTeGrad/Lab1_HAN) | self-attention and HAN (Hierarchical Attention Network) architecture | 
|                | [Lab2](AlTeGrad/Lab2_TransferLearning) | Transfer learning on transformer architecture | 
|                | [Lab3](AlTeGrad/Lab3_Tokenize) | Using **Fairseq and HuggingFace** transformers to finetune pretrained language models | 
|                | [Lab4](AlTeGrad/Lab4_GraphMining) | Spectral Clustering for graphs; Graph Classification using Graph Kernels | 
|                | [Lab5](AlTeGrad/Lab5_DeepWalk_GCN) | DeepWalk algorithm & node embedding & Graph neural network (GNN) | 
|                | [Lab6](AlTeGrad/Lab6_GNN2) | Graph attention network (GAT) &  Graph Classification with deep learning | 
|                | [Lab7](AlTeGrad/Lab7_LearningWithProteins) | DeepSets model & protein classification with GNN | 
|                | [Kaggle challenge](https://github.com/yunhao-tech/altegrad_challenge_2023) | Kaggle challenge: use sequential and structural information to classify protein into 18 classes.
| Causal Inference (IPP) | [Lectures & Labs](Causal%20Inference) | Notebooks in lectures and labs. See the summary in Readme | 
|  | [Final project](https://github.com/yunhao-tech/Counterfactual-Fairness) | Reproduction of paper: [Counterfactual Fairness](https://arxiv.org/abs/1703.06856) to study in machine learning **the fairness using causal inference** | 
| Bootstrap (ENSAE) | [TD1](Bootstrap/TD1) | Application of **Jackknife** to estimate the asymptotic variance (Ex.1) and bias (Ex.2) of estimators | 
|  | [TD2](Bootstrap/TD2) | Application of **Bootstrap** to estimate the bias (Ex.2) and variance (Ex.1, possibly to use Boostrap of Boostrap) of an estimator/statistic | 
|  | [TD3](Bootstrap/TD3) | The exam of last year |
| Sequential Monte-Carlo (ENSAE) | [Final project](https://github.com/yunhao-tech/Sequantial-Monte-Carlo-project) | Employ the SMC methods in Dropout layer of neural network in adaptation stage, in order to replace the fine-tuning | 
| Computer Vision (Telecom) | [Lab1](Computer%20vision/TP1) | self-attention and HAN (Hierarchical Attention Network) architecture | 
|                | [Lab2](Computer%20vision/TP2) | Feature detection (Harris corner detection) & Motion estimation (block matching) & Segmentation (algorithm of Otsu + region-growing based algorithm) | 
| Data streaming (IPP) | [Lab1](Data%20streaming/lab1) | Discovery of River: like sklearn, but it focus on online machine learning | 
|                | [Lab2](Data%20streaming/lab2-Kafka) | Using Docker and Kafka to analyse streaming tweets | 
|                | [Final project: Continual GNN](https://github.com/yunhao-tech/ContinualGNN) | Refactor the original codes in a paper studying streaming GNN via continual learning  |
| Statistic Bayesian (ENSAE) | [DM1](Statistiques%20Bayesians/DM1/Gibbs%20MCMC.ipynb) | Application of MCMC Gibbs sampler to inference parameters based on 'a proteriori' probability | 
|                            | [Final project](https://github.com/yunhao-tech/Bayesian-Inference-project) | Using Gibbs sampling and DMC-IS(direct monte carlo with importance sampling) to reproduce some results in [this paper](https://projecteuclid.org/journals/bayesian-analysis/volume-5/issue-1/Hierarchical-Bayesian-analysis-of-the-seemingly-unrelated-regression-and-simultaneousequations/10.1214/10-BA503.full)  |
| Practical Machine learning (IPP) | [Session 1](IPP-Practical%20Machine%20learning/Session%201/ML_TP1.ipynb) | analyse on several unsupervised machine learning methods: **K-means, GMM, PCA, t-SNE** | 
|                                | [Session 2](IPP-Practical%20Machine%20learning/Session%202/ML_TP2.ipynb) | reguralised regression, variable selection, nonlinear **regression**, on a dataset from the Brain Computer Interface competition | 
|                                | [Session 3](IPP-Practical%20Machine%20learning/Session%202/ML_TP3.ipynb) | comparison between Bayesian decision, linear and nonlinear **classification**, on MNIST dataset and another one about diabetes | 
| Deep learning (IPP) | [Lab 1](IPP-Deep%20learning/TP1_MLP_by_hand.ipynb)  | implement MLP from scratch |
|                   | [Lab 2](IPP-Deep%20learning/TP2_MLP_by_pytorch.ipynb)  | implement MLP using pytorch |
|                   | [Lab 3](IPP-Deep%20learning/TP3_RNN_ManyToOne.ipynb)  | RNN (Many-to-one) |
|                   | [Lab 4](IPP-Deep%20learning/TP4_RNN_LanguageModel.ipynb)  | a simple language model |
|                   | [Lab 5](IPP-Deep%20learning/TP5_CNN_part_1.ipynb)  | build CNN for image recognition, using Pytorch |
|                   | [Lab 5](IPP-Deep%20learning/TP5_CNN_part_2.ipynb)  | visualisation of CNN: Deep Dream algorithm; Adversarial examples |
| MAP566 Statistics in Action (X-3A) | [Homework 1](MAP566-Statistics%20in%20Action%20(R)/Homework%201/homework_1.Rmd)  | Hypothesis testing |
|                                | [Homework 1](MAP566-Statistics%20in%20Action%20(R)/Homework%202/homework_2.Rmd)  | implement MLP using pytorch |
|                                | [TP3](MAP566-Statistics%20in%20Action%20(R)/TD3-polynomial-regression.Rmd)         | Polynomial regression model |
|                                | [TP4](MAP566-Statistics%20in%20Action%20(R)/TD4-nonlinear-regression.Rmd)         | Nonlinear regression model |
|                                | [TP5](MAP566-Statistics%20in%20Action%20(R)/TD5-linear-mixed-model.Rmd)         | linear mixed model |
|                                | [TP6](MAP566-Statistics%20in%20Action%20(R)/TD6-nonlinear-mixed-model.Rmd)         | non linear mixed model |
|                                | [TP7](MAP566-Statistics%20in%20Action%20(R)/TD7-mixture-models.Rmd)         | mixture models |
|                                | [TP8](MAP566-Statistics%20in%20Action%20(R)/TD8-graph-partionning.Rmd)         | Graph Clustering: Spectral and hierarchical methods |
|                                | [TP9](MAP566-Statistics%20in%20Action%20(R)/TD9-stochastic-blockmodels.Rmd)         | Graph Clustering: Stochastic Blockmodels |
|MAP556 Monte Carlo Methods (X-3A)  | [TP1](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP1.ipynb)         | Simulation of random variables + Law of large numbers + Central limit theorem |
|                                | [TP2](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP2.ipynb)          | Serveral methods of variance reduction: **control variates, antithetic sampling, stratification** |
|                                | [TP3](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP3.ipynb)          | Variance reduction through importance sampling |
|                                | [TP5](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP5.ipynb)          | Using Empirical Regression to approximate conditional expectation (in a context of finance) |
|                                | [TP6](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP6.ipynb)          | Generative Adversarial Network (GAN) |
|                                | [TP8](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP8.ipynb)          | Simulate processes of Brownian motion (eg. process of Ornstein-Uhlenbeck) and their Euler scheme |
|                                | [TP9](MAP556-Monte%20Carlo/MAP556-Monte%20carlo%20TP/MAP556_TP9.ipynb)          | Multi-level Monte-Carlo method (MLMC) |
|                                | [Challenge1](MAP556-Monte%20Carlo/Challenge%201/Solution.py)  | simulate E(f(G)), f is reasonably regular|
|                                | [Challenge2](MAP556-Monte%20Carlo/Challenge%202/controle_etudiant.py)  | play Angry Bird! Try to give a control on velocity to the bird facing a random wind|
|MAP553 Machine learning (X)     | [TP1](MAP553-Machine%20learning/MAP553_tp1.ipynb)         | implement several optimization algorithms: **GD, AGD, CGD, SGD, SAG, SVRG** |
|                                | [Project](MAP553-Machine%20learning/MAP553_project/autoML_MAP553.ipynb)     | a classic dataset of tree cover type classification, using auto machine learning, 2nd in Kaggle competition |
|Reinforcement learning (X-2A)   | [Lab3](Reinforcement%20Learning%20algos/lab_rl3_value_iteration.ipynb) | Dynamic Programming - Value Iteration |
|                                | [Lab4](Reinforcement%20Learning%20algos/lab_rl4_policy_iteration.ipynb) | Dynamic Programming - policy iteration |
|                                | [Lab5](Reinforcement%20Learning%20algos/lab_rl5_temporal%20difference.ipynb) | Temporal Difference |
|                                | [Lab6](Reinforcement%20Learning%20algos/lab_rl6_SARSA_QLearning.ipynb) | Q table - SARSA and Q Learning |
|                                | [Lab7](Reinforcement%20Learning%20algos/lab_rl7_PolicyGradient.ipynb) | Policy Gradient |
|INF580 Large scale mathematical optimization (X-3A) | [Project](INF580-Large%20scale%20mathematical%20optimization/INF580.pdf) | combine random projection and linear programming. Retrieve solutions from projected problem and dual projected problem, compute primal solution. Compare their feasibility error and compute time |
|MAP433 statistiques (X-2A)         | [TP1](MAP433-statistiques/EN1/EN1.ipynb)         | Estimation parametrique. Loi de Poisson pour modéliser le nombre de buts marqués par une équipe de football |
|                                | [TP2](MAP433-statistiques/EN3/EN3.ipynb)          | Test de Cramer-von Mises|
|                                | [TP3](MAP433-statistiques/EN5/EN5.ipynb)          | Transformation de stabilisation de la variance |
|                                | [Homework1](MAP433-statistiques/DM1/DM1.ipynb)   | Estimation coefficients and interpretation (linear regression) | 
|                                | [Homework2](MAP433-statistiques/DM2/DM2.ipynb)   | a test asymptotic on regression coefficients |
|                                | [Homework3](MAP433-statistiques/DM3/DM3.ipynb)   | Classification by **KNN** |
|MAP432 Markov & martingale (X-2A) | [Project](MAP432---Recruit_simulé/Recruit_simulé.ipynb)     | L'algorithme du **recuit simulé**, pour résoudre des problèmes d'optimisation non convexe. On s'intéresse ici à une application au problème du voyageur de commerce |
|MAP435 optimisation (X-2A)         | [optimisation **sans contrainte**](MAP435-optimisation/optimisation_sans_contrainte.ipynb)  | Algorithme de gradient à pas fixe |
||  | Algorithme du gradient pas optimal (le cas de fonction quadratic) |
||  | Algorithme de Nesterov (fonctions convexes) |
||  | Algorithme de Nesterov (fonctions fortement convexes) |
||  | Algorithme du gradient conjugué |
||  | Algorithme de Newton |
||  | Analyse de vitesse de convergence et comparer les algos |
||  | Quelques contre-exemples |
|            | [optimisation **avec contraintes**](MAP435-optimisation/optimisation_avec_contraintes.ipynb)  | Algorithme du gradient projeté |
|            |   | Algorithme d'Uzawa |
|            |   | Méthode de pénalisation |
|            |   | Algorithme du Lagrangien augmenté |
